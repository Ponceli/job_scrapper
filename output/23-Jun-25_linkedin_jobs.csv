date_time,search_keyword,search_count,job_id,job_title,company,location,remote,update_time,applicants,job_pay,job_time,job_position,company_size,company_industry,job_details,job_url
23Jun2025-17:06:28,Data Scientist,0,4253310620,Senior Data Scientist,SoTalent,Estados Unidos,false,hace 4 horas,100,,,,,,"Acerca del empleo Job Title: Senior Data Scientist Location: Remote Type: Full time  We’re looking for a Senior Data Scientist with a strong background in AI, machine learning, and advanced analytics to join a forward-thinking, collaborative team. In this role, you’ll lead high-impact data initiatives, design intelligent solutions, and work cross-functionally to solve real-world business problems with data.  What You’ll Do: Build and deploy machine learning models and data pipelines Apply advanced analytical techniques to drive business decisions Collaborate with engineering, product, and analytics teams Evaluate new technologies and tools in AI/ML Turn complex data into clear, actionable insights  What We’re Looking For: 6+ years of hands-on data science experience Proficient in Python, R, SQL, and modern visualization tools (e.g., Power BI, Tableau) Strong understanding of cloud-based platforms and data systems Experience delivering end-to-end ML solutions in production Excellent communication and collaboration skills  Nice to Have: Experience with GenAI or emerging AI technologies Familiarity with agile methodologies and experimentation frameworks  What You’ll Get: Competitive benefits including healthcare, retirement plans, and generous PTO Tuition reimbursement and professional development support Paid volunteer time and inclusive employee resource groups A supportive, flexible work culture focused on growth",https://www.linkedin.com/jobs/view/4253310620/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=4V11uLfNv5ej6ik8a3vh%2BA%3D%3D&trackingId=v%2FDRe%2FOUa1Tq2yjXUWAo%2Fw%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:06:44,Data Scientist,0,4255315046,Sr. Data Analyst,Gravity IT Resources,Estados Unidos,false,hace 3 horas,100,,,,,,"Acerca del empleo Job Title:  Sr Data Analyst  Location: Remote Job-Type: 12-month contract  Our client is a $20B, Fortune 200 company in the automotive retailing, distribution and services industry. They are currently ranked in the Top 20 companies to work for in the US by Fortune Magazine, Top 20 best companies for diversity and have an amazing work culture and impressive long-term growth prospects.  Job Summary:  The Sr. Data Analyst will be responsible developing an understanding of the relationship of the data that supports business processes and applications. The analyst will gain insights into the data and the movement of the data from data entry to final data destinations in the back-end files and reporting systems. Candidates must possess the technical capabilities to extract, retrieve and analyze data on their own. Be able to dive in and research new and complex data sources to understand how the data is moving in the system and how the data is stored to identify any anomalies that may exist. The analyst should possess strong interpersonal communication skills to be able to bridge the gap between business and technical users to relay both business and technical concepts as it relates to data. The analyst will be required to document findings and ensure that mappings and lineage are well documented and stored into knowledge storage repositories. This assignment is for a project that will interact with business and IT to understand business processes and the related data. Must be able to be on a hybrid schedule (Deerfield Beach office) with flexibility to be in the office more, as required.   Job Requirements:   o 5+ years of experience as a Data Analyst   o Senior level proficiency with SQL o Ability to independently complete data lineage projects and source to target mapping   o BA or BS Degree in Data Analytics, Computer Science or a related field   o Excellent communication skills and ability to effectively communicate with technical and business teams   o Executive / business presence  o Ability to develop trusted partnerships with stakeholders and other employees.",https://www.linkedin.com/jobs/view/4255315046/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=4V11uLfNv5ej6ik8a3vh%2BA%3D%3D&trackingId=GVJ%2FY2DP2%2BQxkf6B7D8rYw%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:07:00,Data Scientist,0,4255195040,Senior Data Scientist,Empiric,Estados Unidos,false,hace 6 horas,100,,,,,,"Acerca del empleo Senior Data Scientist / Machine Learning Engineer  Location: Remote  Contract: 6 months initial  Rate: $75-$90 per hour   Impact of Your Role As a Senior Data Scientist / Machine Learning Engineer, you will: Develop innovative solutions utilizing Large Language Models (LLMs) on customer data, including Retrieval-Augmented Generation (RAG) and agentic architectures, to enhance enterprise knowledge repositories. Implement natural language querying for structured data and facilitate content generation. Expand customer data science capabilities by applying best practices in MLOps to ensure successful deployment across diverse domains. Provide strategic guidance to data teams on architecture, tools, and best practices in data science. Offer technical mentorship to the broader Machine Learning Subject Matter Expert community. Required Experience: Proficiency in advanced natural language processing techniques, including vector databases, LLM fine-tuning, and deployment using tools such as HuggingFace, Langchain, and OpenAI. 5 to 6+ years of hands-on experience in production data science, utilizing tools such as pandas, scikit-learn, gensim, nltk, and TensorFlow/PyTorch. Proven track record of building production-grade machine learning solutions on cloud platforms like AWS, Azure, or GCP. Strong ability to communicate complex technical concepts effectively to both technical and non-technical audiences. Experience with Apache Spark™ for processing large-scale distributed datasets. Familiarity with the Databricks platform. At least 2 years of customer-facing experience in a pre-sales or similarly technical role.",https://www.linkedin.com/jobs/view/4255195040/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=4V11uLfNv5ej6ik8a3vh%2BA%3D%3D&trackingId=dCkXglLihyW8DZNM5EhGuA%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:07:18,Data Scientist,0,4255302863,Senior Data Scientist,Acumatica,"Seattle, WA",false,hace 4 horas,100,,,,,,"Acerca del empleo Company Description  Acumatica is a company on a mission. We are a leading innovator in cloud ERP with customers located around the world. But don’t take our word for it—read what analysts like G2 and Info-Tech have to say about us.  Acumatica is a leading provider of cloud business management software that empowers small and mid-size businesses to unlock their potential and drive growth. Built on the world’s best cloud and mobile technology and a unique customer-centric licensing model, Acumatica delivers a suite of fully integrated business management applications, such as Financials, Distribution, CRM, and Project Accounting, on a robust and flexible platform. In an interconnected world, Acumatica enables customers to take full control of their businesses, play to their organizations’ unique strengths, and support their clients by following them anywhere on any device.  Acumatica’s culture is casual and high-energy. We are passionate about our product and our mission, and we are loyal to each other and our company. We value work/life balance, efficiency, simplicity, freakishly friendly customer service, and making a difference in the world. Acumatica offers exceptional professional and financial growth potential.  To learn more about Acumatica’s mission, please visit: http://www.acumatica.com.  Job Description  This is an exciting opportunity for a senior data scientist to play a strategic role in shaping how data is used to drive decisions, insights, and innovation across our business, partners and customers ecosystem. In this role you will work closely with our business teams to understand their goals, challenges, and use cases – translating them into actionable data and analytics requirements.  You will help define both the analytics and data engineering dimensions of our data platform strategy. On the analytics side, you will define key business metrics, design data models, and develop insights and dashboards that inform critical tactical & strategic decisions. You will collaborate with data engineering to ensure the underlying data architecture, pipelines, and structures are aligned with analytical needs.  You will also help uncover and shape opportunities for advanced analytics and machine learning, including use case discovery, prototyping, and advising on practical deployment for our business teams. You will have the opportunity to help our business stakeholders move beyond reactive reporting to forward-looking, data-driven decision-making with scalable, value-driven advanced analytics.  Key Responsibilities  Business Partnership & Requirements Gathering Work with cross-functional business team to understand key objectives and challenges of each business function Translate business goals into data and analytical requirements for data engineering and data analytics teams Proactively identify areas where analytics and machine learning can provide enhanced value and insights beyond current scope Communicate complex findings in a clear, actionable manner to both technical & non-technical stakeholders Analytical Leadership Design, build, and implement advanced analytics models (e.g. forecasting, classification, segmentation) to support internal and customer facing use cases Serve as a thought leader on data science best practices, emerging trends, and innovation opportunities Collaboration & Enablement Work closely with data engineers to ensure data pipelines and structures align with analytical needs Help define and maintain a roadmap for analytics and ML capabilities aligned with business and product goals Mentor rising data scientists and analysts Qualifications  Education  Bachelor’s or Master’s degree in Computer Science, Data Science, or a related field  Experience  7+ years of experience in data science or advanced analytics, preferably within SaaS ERP or enterprise software Strong experience working with business and technical teams Proven ability to translate business analytics needs into well-defined & structured analytics deliverables Strong communication skills with ability to influence and educate stakeholders at all levels Experience working with large, complex datasets and modern cloud data platforms (e.g. Snowflake, Data Bricks, Big Query, AWS Spark, Dremio, Apache Iceberg) Ability to be a lead for a team of 3-5 data analysts  Technical Skills  Hands-on experience with Python or R, Advanced SQL, and machine learning libraries Strong data modeling experience Strong knowledge of ETL/ELT pipelines, streaming data, and orchestration frameworks Understanding of data governance frameworks & compliance standards Experience integrating machine learning workflows into lakehouse environments  Additional Information  Acumatica is an Affirmative Action and Equal Opportunity Employer/Veterans/Disabled. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability.  If you have a disability and you believe you need a reasonable accommodation in order to search for a job opening or to submit an online application, please e-mail HR@acumatica.com. This email is created exclusively to assist disabled job seekers whose disability prevents them from being able to apply online. Only emails sent for this purpose will be returned. Emails sent for other purposes, such as following up on an application or technical issues not related to a disability, will not receive a response.  For this role, the salary range is $140,000-160,000 annually. This range represents the low and high end of the salary range for this job and may vary based on location. The actual salary offer will carefully consider a wide range of factors, including skills, qualifications, experience and other relevant elements.  At Acumatica, certain roles are eligible for additional rewards, including annual bonus and stock. These awards are allocated based on individual performance. In addition, certain roles also have the opportunity to earn sales incentives based on revenue or utilization, depending on the terms of the plan and the employee’s role.",https://www.linkedin.com/jobs/view/4255302863/?eBP=BUDGET_EXHAUSTED_JOB&refId=4V11uLfNv5ej6ik8a3vh%2BA%3D%3D&trackingId=kQbT224EQml%2BMbwdwHZu0A%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:07:35,Data Scientist,0,4255131804,Data Science & ML Engineer,TrueNorth®,Reino Unido,false,hace 11 horas,100,,,,,,"Acerca del empleo One of our clients are a leading E-commerce company are looking for an DS/ML engineer to join them for an initial 6 months contract.  Details of this contract as follows:  Location: Remote Duration: 6 months, high likelihood of extension Rate: £400 - £450 per day Status: Outside IR35 Start date: Ideally immediate (they may be able to wait up to 4 weeks for someone to start) Skills required: NLP, Modelling, Python, SQL, Databricks, Anomaly Detection Desirable: Image Classification, Airflow, Pyspark   You will be joining their customer security team, which uses machine learning to detect anomalies and erroneous activity on customer accounts. You will be building models to help detect any unwanted account takeovers by cyber criminals and build customer trust.  If this is of interest, then please apply today or send through an updated CV to:  david@truenorthgroup.io",https://www.linkedin.com/jobs/view/4255131804/?eBP=BUDGET_EXHAUSTED_JOB&refId=4V11uLfNv5ej6ik8a3vh%2BA%3D%3D&trackingId=gHm5RS2lGWkn4UfS%2BGAFgw%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:07:54,Data Scientist,0,4252676518,Senior Data Analyst: 100% remote [32334],,Estados Unidos,false,hace 9 horas,100,,,,,,"Acerca del empleo The company is a fast-growing, independently-owned real estate software firm that serves as a trusted technology partner to over 500,000 top brokerages, agents, and teams. Their branded portfolio, BoldTrail, includes BoldTrail (Front Office), BoldTrail BackOffice and BoldTrail Recruit, solutions that create a complete tech ecosystem for clients, and deliver seamless end-to-end operations to scale success at any level. With an accomplished leadership team and its talented staff, the company brings the resources, scale, and vision to deliver ongoing innovation and success to their growing customer base. We are seeking a Senior Data Analyst with expertise in product analytics, behavioral analysis, and predictive analytics to join our team. The ideal candidate will combine strong statistical knowledge with business acumen to drive product and business decisions through data-driven insights. This role is crucial in empowering teams with the data they need to make informed decisions.  Responsibilities Lead the analysis of user behavior and trends by leveraging advanced statistical methods and predictive models to generate insights into engagement and retention. Develop and standardize product metrics, aligned with company OKRs; utilizing cohort analysis to better understand user lifecycles and influence on customer lifetime value. Design and execute experimental strategies, including A/B tests to measure the impact of product changes, directly influencing product roadmap decisions. Create and maintain scalable reporting solutions that democratize access to key metrics, while ensuring data accuracy and integrity. Collaborate with cross-functional teams to define, measure, and capture data, transforming business questions into critical insights. Deliver clear, concise reports and visualizations that effectively communicate insights to both technical and non-technical audiences. Lead the analysis for customer success stories and sales enablement by creating data-backed proof points for case studies and sales narratives Mentor junior analysts while documenting and standardizing best practices for data analysis and reporting across the organization. Qualifications: Bachelor’s degree in Data Science, Statistics, Mathematics, Computer Science, or a related field (Master's degree preferred). 3-5+ years of hands-on experience with product and customer analytics with a specialty in B2B/SaaS (i.e., product usage and engagement analysis, customer lifecycle metrics)  Strong proficiency in data analysis tools such as SQL and programming languages such as Python or R, with demonstrated ability to write efficient queries and build reproducible data pipelines to support statistical and analytical models. Experience with data visualization and BI tools for creating intuitive dashboards (Tableau strongly preferred) Experience with modern data stacks including cloud-based databases (Redshift), ETL/Pipeline orchestration tools (Airflow), product analytics platforms (Segment, Amplitude) and version control/database tools (Github). Experience with experimental design and A/B testing methodologies Proven ability to lead and manage multiple initiatives from problem definition, defining success metrics, designing the experiment and communicating actionable results.  Strong understanding of data governance and data quality principles. Excellent problem-solving skills and attention to detail. Possesses strong written and verbal communication skills in order to collaborate effectively across various teams and organizational levels. Experience in the Real Estate industry is a plus.  We offer a competitive total rewards package including: Competitive salary 3 Medical plans to choose from - 1 PPO and 2 HDHPs 2 Dental plans to choose from Vision HSA - company-funded FSAs - healthcare, limited purpose, dependent care Short-Term Disability - company-paid Long-Term Disability - company-paid Basic Employee Life Insurance - company-paid Voluntary Dependent Life Insurance Voluntary Accident Insurance Voluntary Critical Illness Voluntary Hospital Indemnity Legal Plan ID Protection Pet Insurance 401(k) Retirement Savings with company match Paid PTO/Vacation/Sick Time 11 company-recognized holidays Company-paid Parental/Disability Leave   In addition, we focus on driving top results providing: Opportunities to grow within our company; Potential to work in a remote setting; Exciting/energetic work environment and fun, creative culture.",https://www.linkedin.com/jobs/view/4252676518/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=4V11uLfNv5ej6ik8a3vh%2BA%3D%3D&trackingId=d8W5OJdjuBKpeBqEa3BuaQ%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:08:10,Data Scientist,0,4255195169,AI/ ML Engineer,Acetech Group Corporation,Estados Unidos,false,hace 6 horas,100,,,,,,"Acerca del empleo Position: AI/ ML Engineer Location:100% Remote Duration: 12+ contract   Job Description We are seeking a talented Java Full Stack Developer with expertise in AI/ML to join our dynamic team.  Responsibilities: Design, develop, and maintain scalable web applications using Java and related technologies. Implement and integrate AI/ML models and algorithms into applications, focusing on technologies such as RAG (Retrieval-Augmented Generation), MCP (Model Compression and Pruning), LlamaIndex, and LangChain. Work with vector databases to manage and query large datasets efficiently. Collaborate with cross-functional teams to define, design, and ship new features. Ensure the performance, quality, and responsiveness of applications. Identify and correct bottlenecks and fix bugs. Stay up-to-date with emerging trends and technologies in fullstack development and AI/ML.   Required Skills & Experience Bachelor's degree in Computer Science, Engineering, or a related field. Proven experience as a Fullstack Developer with a strong proficiency in Java. Hands-on experience with AI/ML technologies, including GenAI, RAG, MCP, LlamaIndex, and LangChain. Experience with vector databases and their applications. Strong understanding of front-end technologies, such as HTML, CSS, and JavaScript. Familiarity with frameworks such as Spring Boot and Hibernate.  Nice to Have Skills & Experience Master's degree in Computer Science or a related field. Experience with cloud platforms such as AWS, Azure, or Google Cloud. Knowledge of DevOps practices and tools. Contributions to open-source projects.",https://www.linkedin.com/jobs/view/4255195169/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=4V11uLfNv5ej6ik8a3vh%2BA%3D%3D&trackingId=h9kFkWjyl7tQSg58R1GVEg%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:08:28,Data Scientist,0,4255203695,"Data Scientist, Entry Level",Jobright.ai,"Oregón, Estados Unidos",false,hace 7 horas,,,,,,,"Acerca del empleo At Jobright, we help high-growth startups hire top talent for their key roles.  The Voleon Group is a technology company focused on applying advanced AI and machine learning techniques to investment management. As a Data Scientist, you will be responsible for deriving insights from complex datasets, ensuring data integrity, and communicating findings to both research staff and executive leadership.  Responsibilities: Design and implement systems to ensure data correctness and monitor data health in data stores and live feeds Proactively identify abnormal production behavior and communicate them clearly to relevant stakeholders Perform extemporaneous analyses on research and production trading systems with leadership Harness financial expertise and statistical analysis to gain actionable insights into our production trading and research systems Design and implement analysis pipelines that automate those analyses found to be valuable for ongoing monitoring  Qualification:  Required: 1+ years of applied end-to-end industry experience, including internships working with complex datasets, including curation, querying, aggregation, exploratory data analysis, and visualization Experience using statistical methods to analyze data, identify patterns, conduct root cause analysis, discover insights, and recommend solutions Ability to frame and answer questions mathematically Ability to infer useful forward-looking directions from results of retrospective analysis Fluency in managing, processing, and visualizing tabular data using a combination of SQL, Pandas, and R Basic software development skills and experience with bash, linux/unix, and git Ability to refine requirements from ambiguous requests to produce reports demonstrating excellence in communication Bachelor’s degree in a quantitative discipline (statistics, biostatistics, data science, computer science, or a related field)  Preferred: Master’s degree in a quantitative discipline Prior industry experience or displayed interest in finance, such as related academic projects, coursework in financial engineering, or industry internships Experience developing in a production-facing environment and familiarity with standard concepts and tooling, e.g., CI/CD, git, Airflow",https://www.linkedin.com/jobs/view/4255203695/?eBP=BUDGET_EXHAUSTED_JOB&refId=4V11uLfNv5ej6ik8a3vh%2BA%3D%3D&trackingId=FFrEr4Lg8dd5FtDt%2BErrWQ%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:08:45,Data Scientist,0,4255098146,Data Scientist,People Prime Worldwide,India,false,hace 8 horas,100,,,,,,"Acerca del empleo About Company: Client is a US-based AI infrastructure company focused on accelerating the development and deployment of AI systems. It connects companies with AI experts and provides end-to-end AI solutions, initially known for its Intelligent Talent Cloud and now offering services for designing, training, and deploying advanced AI systems. Client also focuses on advancing frontier AI model capabilities and building real-world AI applications.   Job Title: Data Scientist with Julia Experience Location: Remote Client: Turing Experience: 3+ yrs Job Type : Contract to hire. Notice Period:- Immediate joiners Only.   Note: Candidate should be comfortable to work for US Shifts/Night Shifts. Interview Mode: Virtual (Two rounds of interviews (60 min technical + 30 min technical & cultural discussion).  Roles and Responsibilities: Strong experience with Julia programming language concepts.  Industry experience and knowledge of code quality, formatting, and best practices of software development. Experience with Julia's testing ecosystem, including unit, integration, and property-based testing.  Familiarity with Julia frameworks and libraries. Knowledge of multi-threading and asynchronous programming in Julia.  Ability to work with architectural patterns and refactor code without introducing regressions. Strong debugging skills, including fixing memory and concurrency issues.  3+ years of overall work experience with 2+ years of relevant experience with Julia. Fluent in conversational and written English communication skills.",https://www.linkedin.com/jobs/view/4255098146/?eBP=BUDGET_EXHAUSTED_JOB&refId=4V11uLfNv5ej6ik8a3vh%2BA%3D%3D&trackingId=y2YVBvzHka4pnNUtCm6Bsg%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:09:00,Data Scientist,0,4255251146,Machine Learning Engineer,Creospan Inc.,Estados Unidos,false,hace 1 hora,100,,,,,,"Acerca del empleo Job Title: ML Engineer Location: Remote Job Duration: 6 months Contract  Job Description: We are seeking skilled engineers to provide daily support for our in-house Gen AI platforms, which serve legal stakeholders. The successful candidates will be responsible for: Building and maintaining the platforms, including refactoring software as needed Troubleshooting and resolving bugs and issues that arise Collaborating with FTEs to share workload and support ongoing projects Ensuring seamless operation of the platforms to meet the needs of our legal stakeholders  Qualifications: 1) Master's/Bachelor's in Computer Science, Math, Physics, Engineering, Statistics or a related quantitative field. 2) 3+ years of experience in the design, deployment and maintenance of Machine learning pipelines and systems. 3) Demonstrable experience in applying Machine Learning algorithms to solve real-world problems 4) 3+ years of experience in design, deployment and maintenance of large data pipelines.",https://www.linkedin.com/jobs/view/4255251146/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=4V11uLfNv5ej6ik8a3vh%2BA%3D%3D&trackingId=uGKiBIBYOwI8x5OWeMzgDg%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:09:18,Data Scientist,0,4255319081,Data Scientist,Wiraa,Estados Unidos,false,hace 2 horas,,,,,,,"Acerca del empleo About The Company  Ulta Beauty (NASDAQ: ULTA) is the largest North American beauty retailer, offering a comprehensive range of cosmetics, skincare, haircare, fragrance, and salon services. With over 25,000 products from 500+ brands and an in-store salon in every location, Ulta is the ultimate beauty destination. Our company thrives on innovation, inclusivity, and empowering both customers and team members through the transformative power of beauty.  About The Role  As a Lead Data Scientist at Ulta Beauty, you will lead high-impact, exploratory data science initiatives at the intersection of innovation and business strategy. You will collaborate across functions to build scalable AI/ML solutions using advanced techniques spanning NLP, computer vision, and generative modeling. Reporting to senior leadership, this position is central to our innovation roadmap and will guide the development and implementation of cutting-edge solutions across the enterprise.  Responsibilities  Lead strategic data science projects aligned with Ulta’s business goals Drive innovation through experimentation, rapid prototyping, and continual learning Mentor and develop a team of data scientists, promoting best practices and technical excellence Architect, implement, and monitor scalable ML models and end-to-end pipelines Perform complex data analysis using structured and unstructured datasets Translate technical insights into clear, actionable recommendations for business partners Present findings to senior executives and stakeholders to influence key decisions Collaborate with teams across engineering, product, marketing, and finance Contribute to MLOps infrastructure and model deployment strategy Evaluate emerging technologies and provide thought leadership across AI initiatives  Qualifications  Master’s or PhD in Computer Science, Mathematics, Physics, Statistics, or a related field 5–7+ years of progressive experience in data science, machine learning, or AI roles Proficiency in Python or C++ and ML frameworks (e.g., scikit-learn, TensorFlow) Deep experience with cloud platforms such as Google Cloud, AWS, or Azure Proven track record of developing and launching ML solutions in production Solid foundation in statistics, machine learning, optimization, and software engineering Strong communication and project leadership skills with business-facing experience Ability to work in fast-paced, cross-functional environments with evolving requirements  Benefits  Competitive salary range: $119,300 – $170,000 per year Eligibility for annual performance-based bonus Paid time off, paid holidays, and wellness days Comprehensive health, dental, vision, life, and disability insurance 401(k) with company match and career development opportunities Access to Ulta Beauty's learning platform and internal mobility programs Opportunities for occasional travel (team collaboration, conferences, vendor engagement)  Equal Opportunity  Ulta Beauty is an equal opportunity employer and is committed to fostering a diverse and inclusive workplace. We consider all qualified applicants, including individuals with arrest or conviction records, in accordance with applicable laws such as the Fair Chance Ordinance in Los Angeles, San Francisco, and New York City.  Aptitudes y experiencia deseables Information Technology",https://www.linkedin.com/jobs/view/4255319081/?eBP=BUDGET_EXHAUSTED_JOB&refId=4V11uLfNv5ej6ik8a3vh%2BA%3D%3D&trackingId=1JSyHxD8Pr%2FDu%2BldqEPQYA%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:09:35,Data Scientist,0,4252387832,Data Scientist / AI / Gen AI / Software Engineer / LLM / ML / Machine Learning / Solutions Architect,BSL Consulting,Estados Unidos,false,Publicado de nuevo hace 3 horas,100,,,,,,"Acerca del empleo Role : Data Scientist (Traditional) Type : Contract Location : Atlanta, GA (Hybrid - 3 Days a week)  Should have developed propensity models within insurance business domain  Should be proficient at Python  Should have prior experience in AWS SageMaker, Databricks, etc.  Should be familiar with NLP techniques (OCR based text extraction, document classification, document summarization)  Palantir Foundry and AIP experience is a plus   Role : Data Scientist (GenAi) Type : Contract Location : Atlanta, GA/NYC (Hybrid - 3 Days a week)  A strong AI and ML engineer with vast experience in ML, DL, NLP and Gen-AI solutions Should have insurance domain experience (specifically Underwriting/Claims) Should be proficient at Python Should have prior experience in AWS SageMaker, Databricks, etc. Should be experienced with NLP techniques (OCR based text extraction, document classification, document summarization) Should have prior experience around LLMs and Gen-AI projects Palantir Foundry and AIP experience is a plus     Role : Full Stack Software Engineer (GenAi SDLC) Type : Contract Location : Charlotte (Hybrid - 3 Days a week) Develop and deploy GenAI-powered features across the full stack (front-end, back-end, databases). Implement and manage CI/CD pipelines optimized for GenAI model deployment. Integrate GenAI models (LLMs, etc.) into existing and new software systems. Ensure data pipelines for GenAI model training and inference are efficient and reliable. Write clean, testable, and well-documented code. Troubleshoot and resolve issues across the full stack and GenAI integrations. Stay current with GenAI advancements and full-stack technologies Proficiency in multiple programming languages (e.g., Python, JavaScript, Java). Strong experience with front-end frameworks (e.g., React, Angular, Vue.js). Solid back-end development skills (e.g., Node.js, Python frameworks like Flask/Django, Spring). Experience with databases (SQL, NoSQL). Familiarity with cloud platforms (AWS). Understanding of SDLC principles and agile methodologies. Hands-on experience integrating AI/ML models into applications. Understanding of Generative AI concepts (LLMs, transformers, etc.).work with Agentic AI framework or other Gen AI models/APIs (e.g., OpenAI, Hugging Face, Google Vertex AI). Experience with containerization (Docker, Kubernetes)   Role : Solution Architect (SDLC) Type : Contract Location : Charlotte (Hybrid - 3 Days a week) Lead and manage a team of developers in a fullstack development environment. * Define the integration strategy for GenAI models (LLMs, etc.) into existing and new systems. Lead the technical vision and roadmap for GenAI solutions within the SDLC. Define data pipelines and infrastructure requirements for GenAI model training, deployment, and monitoring. Ensure alignment of GenAI solutions with business needs and technical feasibility. Collaborate with engineering, data science, and product teams to define solution requirements. Develop, enhance, and maintain applications using Python, Angular, and Java. Integrate and work with Agentic AI framework or other Gen AI models/APIs (e.g., OpenAI, Hugging Face, Google Vertex AI). Design and deploy scalable microservices-based solutions. Collaborate with Product Owners, DevOps, and QA teams in an Agile environment. Ensure code quality through peer reviews, test automation, and best practices. Contribute to architectural decisions and technical strategies.",https://www.linkedin.com/jobs/view/4252387832/?eBP=BUDGET_EXHAUSTED_JOB&refId=4V11uLfNv5ej6ik8a3vh%2BA%3D%3D&trackingId=r4LeoDBnVyte2zVjGV23Pg%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:09:50,Data Scientist,0,4255211072,Analytics Manager,Sibitalent Corp,"Nueva York, Estados Unidos",false,hace 6 horas,93,,,,,,"Acerca del empleo Role - BI/Analytics Manager  Location – Remote - Only or Self corp please   Must have leader/ manager experience SQL, Power BI, Fabric, Healthcare and Snowflake exp.",https://www.linkedin.com/jobs/view/4255211072/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=4V11uLfNv5ej6ik8a3vh%2BA%3D%3D&trackingId=VcNGo4pSiOnQ9rUzbwsXtQ%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:10:06,Data Scientist,0,4255361312,Machine Learning Engineer,Tailored Management,"California, Estados Unidos",false,hace 32 minutos,37,,,,,,"Acerca del empleo Job Title: Software Engineer III (Machine Learning) Location: Remote Duration: 6 month (chance of extension) Pay rate: $100/hr on W2 Start Date: ASAP  We are seeking skilled software engineers to provide daily support for our in-house Gen AI platforms, which serve legal stakeholders. The successful candidates will be responsible for: Building and maintaining the platforms, including refactoring software as needed Troubleshooting and resolving bugs and issues that arise Collaborating with FTEs to share workload and support ongoing projects Ensuring seamless operation of the platforms to meet the needs of our legal stakeholder Apply expertise in Machine Learning, AI, statistics, and data exploration and analysis in leveraging internal operations data to solve critical problems for multiple legal teams Support cross-functional relationships with Engineers and Data Scientists in order to shape deliverables with a balance of technical rigor and strategic consideration Design, deployment and maintenance of ML Pipelines  Must have qualifications: 7+ years of Python and SQL experience  3+ years of experience in the design, deployment and maintenance of Machine learning pipelines and systems. 3+ years of experience in design, deployment and maintenance of large data pipelines. Demonstrable experience in applying Machine Learning algorithms to solve real-world problems Master's/Bachelor's in Computer Science, Math, Physics, Engineering, Statistics or a related quantitative field.   Note: Qualified candidate must undergo Python and SQL Coderpad assessment",https://www.linkedin.com/jobs/view/4255361312/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=4V11uLfNv5ej6ik8a3vh%2BA%3D%3D&trackingId=l%2FaD47zOGrfwJqU7OTPwNg%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:10:23,Data Scientist,0,4255214202,Senior Data Engineer,Harnham,"Inglaterra, Reino Unido",false,hace 5 horas,42,,,,,,"Acerca del empleo DATA ENGINEER - DBT / AIRFLOW / DATABRICKS  4-MONTH CONTRACT   £450-550 PER DAY OUTSIDE IR35 This is an exciting opportunity for a Data Engineer to join a leading media organisation working at the forefront of data innovation. You'll play a key role in designing and building the data infrastructure that supports cutting-edge machine learning and LLM initiatives. Expect to work closely with data scientists, helping to power intelligent content and audience tools across the business. THE COMPANY  A major player in the media sector, this company is investing heavily in its data capabilities to support everything from real-time analytics to AI-powered content discovery. The environment is collaborative, fast-moving, and focused on innovation. With strong engineering foundations already in place, they're looking for a contractor to accelerate delivery of critical pipelines and platform improvements. THE ROLE  You'll join a skilled data team to lead the build and optimisation of scalable pipelines using DBT, Airflow, and Databricks. Working alongside data scientists and ML engineers, you'll support everything from raw ingestion to curated layers powering LLMs and advanced analytics.  Your responsibilities will include: Building and maintaining production-grade ETL/ELT workflows with DBT and Airflow Collaborating with AI/ML teams to support data readiness for experimentation and inference Writing clean, modular SQL and Python code for use in Databricks Contributing to architectural decisions around pipeline scalability and performance Supporting the integration of diverse data sources into the platform Ensuring data quality, observability, and cost-efficiency KEY SKILLS AND REQUIREMENTS Strong experience with DBT, Airflow, and Databricks Advanced SQL and solid Python scripting skills Solid understanding of modern data engineering best practices Ability to work independently and communicate with technical and non-technical stakeholders Experience in fast-paced, data-driven environments DESIRABLE SKILLS Exposure to LLM workflows or vector databases Experience in the media, content, or publishing industries Familiarity with cloud data platforms (e.g., AWS or Azure) Knowledge of MLOps and ML/data science pipelines HOW TO APPLY  To register your interest, please apply via the link or get in touch with your CV.",https://www.linkedin.com/jobs/view/4255214202/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=4V11uLfNv5ej6ik8a3vh%2BA%3D%3D&trackingId=J7FELmluwNlqm9ESOXl7rg%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:10:40,Data Scientist,0,4255359256,Data Scientist,Hirenza,Estados Unidos,false,hace 40 minutos,,,,,,,"Acerca del empleo About The Company  T-Mobile USA, Inc. is a leading telecommunications provider committed to delivering innovative wireless solutions and exceptional customer service. As part of the broader T-Mobile Group, the company has established itself as a disruptive force in the industry, emphasizing a customer-centric approach and a culture of continuous innovation. T-Mobile's mission is to connect people, empower communities, and foster technological advancements that enhance everyday life. With a focus on diversity, inclusion, and sustainability, T-Mobile strives to create a dynamic work environment where talent can thrive. The company invests heavily in its employees through comprehensive benefits, professional development opportunities, and a vibrant corporate culture that encourages growth and collaboration.  About The Role  The Strategy and Corporate Development (S&CD) team at T-Mobile is seeking a highly skilled Principal Data Scientist to join their ranks. This pivotal role involves building data-driven strategic insights that influence key business decisions and drive transformational change across the organization. The ideal candidate will possess a blend of strategic thinking and technical expertise, capable of navigating complex data environments and translating analytical findings into actionable insights for senior leadership. As a key member of the internal consulting team, you will work closely with executive leaders and cross-functional teams to solve ambiguous problems, such as customer coverage optimization and network planning, leveraging advanced machine learning, AI, and statistical models. This role offers an exciting opportunity to impact the company's strategic direction while working on innovative projects that shape the future of telecommunications.  Qualifications  The ideal candidate will have a strong educational background with a Bachelor's Degree in a quantitative discipline such as mathematics, statistics, economics, computer science, physics, or engineering; an advanced degree (Master's or PhD) is preferred. Candidates should possess 7-10 years of industry experience in predictive modeling, data science, and analysis, with a proven track record of deploying machine learning models and developing deep learning solutions. Extensive experience with data scripting languages such as SQL, Python, or R, and familiarity with relational databases are essential. Additionally, the candidate should have 4-7 years of experience in communicating complex technical concepts to diverse audiences, along with expertise in statistical methods, advanced modeling techniques, and big data architectures like Hadoop, Spark, and Kafka. Experience within the telecom industry and management consulting is advantageous. Strong analytical skills, proficiency in mathematics, and the ability to work collaboratively across teams are critical for success in this role.  Responsibilities  Extract, prepare, and model large, complex datasets using a combination of machine learning, statistical, and mathematical techniques to uncover actionable insights. Identify emerging technologies and tools that can enhance data integration, analytics capabilities, and overall data science workflows. Lead the development, implementation, and standardization of advanced analytics and modeling toolkits for the data science team to ensure consistency and best practices. Partner with data engineering teams to define data management strategies, architecture, pipelines, and governance frameworks that support robust modeling environments. Provide strategic guidance and mentorship to data science and measurement science teams, ensuring methodological rigor and innovative approaches. Communicate complex data insights clearly and effectively to senior leaders and stakeholders through verbal presentations, written reports, and compelling data visualizations. Collaborate with cross-functional teams to identify business challenges, structure problem-solving approaches, and deliver impactful solutions. Contribute to ongoing projects and initiatives as directed by business management, supporting the company's strategic objectives and operational improvements.  Benefits  T-Mobile offers a comprehensive Total Rewards Package designed to support the well-being and professional growth of its employees. Benefits include competitive base salary and performance-based bonuses, along with wealth-building opportunities such as stock grants, employee stock purchase plans, and a 401(k) retirement plan. Employees have access to free, year-round financial coaching to assist with personal financial planning. The company provides extensive health benefits, including medical, dental, and vision insurance, along with flexible spending accounts. Paid time off includes about four weeks for full-time employees, with additional paid parental and family leave, and up to 12 paid holidays annually. T-Mobile also offers tuition assistance, family building benefits, childcare subsidies, disability and life insurance options, and voluntary insurance plans. Additional perks include discounts on mobile services, home internet, pet insurance, and transit programs, reflecting the company's commitment to employee satisfaction and work-life balance.  Equal Opportunity  T-Mobile USA, Inc. is an Equal Opportunity Employer. All employment decisions are made without regard to age, race, ethnicity, color, religion, creed, sex, sexual orientation, gender identity or expression, national origin, religious affiliation, marital status, citizenship status, veteran status, disability, or any other characteristic protected by federal, state, or local law. The company is committed to fostering an inclusive environment where all individuals are valued and respected. If you require reasonable accommodation during the application or interview process due to a disability, please contact us by email or phone. T-Mobile values diversity and believes that talent comes in many forms, contributing to our innovative and dynamic workplace culture.  Aptitudes y experiencia deseables Data Science, Machine Learning, Statistical Analysis, Data Analysis, Data Engineering, Predictive Modeling",https://www.linkedin.com/jobs/view/4255359256/?eBP=BUDGET_EXHAUSTED_JOB&refId=4V11uLfNv5ej6ik8a3vh%2BA%3D%3D&trackingId=ZNVKQulgaqyBYtaEmEGUqw%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:10:58,Data Scientist,0,4252652803,Associate Data Scientist,Why Hiring,Canadá,false,hace 14 horas,100,,,,,,"Acerca del empleo At Why Hiring, we believe in the power of connecting talented individuals with incredible remote job opportunities. Our mission is to simplify the job search process and empower professionals to find fulfilling roles that align with their skills and passions, regardless of geographical constraints.   In this role, you will collaborate with top-tier professionals in a setting that nurtures both creativity and career development. Engaging with multimodal datasets, you'll be instrumental in developing pioneering solutions on an extended roadmap.  Responsibilities:  Utilize cutting-edge algorithms and libraries to address complex client challenges. Design and deploy advanced AI features. Analyze extensive multi-modal datasets to derive insights and features for downstream models and applications. Develop and refine traditional machine learning models for both supervised and unsupervised learning, as well as advanced AI models for tasks including image-to-text, text-to-image, question-answering, and summarization. Keep up-to-date with the latest advancements in AI research and integrate these into our products and services. Effectively communicate technical challenges and solutions to both technical and non-technical stakeholders and team members.  Qualifications: Proficiency in Python programming. Knowledge of agile software development practices, including code reviews, unit testing, and version control with git. Practical experience with libraries and frameworks such as Hugging Face, NLTK, SpaCy, TensorFlow, PyTorch, and Scikit-Learn. Strong grasp of deep learning architectures, including transformers, recurrent and convolutional neural networks, and attention mechanisms.",https://www.linkedin.com/jobs/view/4252652803/?eBP=BUDGET_EXHAUSTED_JOB&refId=4V11uLfNv5ej6ik8a3vh%2BA%3D%3D&trackingId=EGOP63VM%2FChdeIXeJjxDjA%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:11:15,Data Scientist,0,4251730580,Data Scientist,NMV Innovation Services,Bulgaria,false,Publicado de nuevo hace 8 horas,52,,,,,,"Acerca del empleo We are looking for a Senior Data Scientist(s) to join a global analytics initiative focused on delivering large-scale machine learning models within a complex financial services environment.  Key Responsibilities: Design, implement, and optimize machine learning models for clustering, classification, and risk-based segmentation Process and analyze complex transactional datasets, enhancing model performance and scalability Conduct advanced statistical modeling, scenario tuning, and parameterization activities Work extensively with Apache Spark (including internals), Python, and Git to develop, test, and operationalize solutions Collaborate closely with data engineering and business teams to ensure smooth integration and continuous model refinement Requirements Master's degree in Data Science or a related discipline 7+ years of hands-on experience in ML/AI model development following completion of the master's degree Deep understanding of clustering and classification algorithms Experience working with structured transactional or behavioral data Proficiency with Apache Spark (including internals), Python, and Git Strong communication skills in English (written and verbal) Experience in financial services, banking, or large-scale transactional environments is a plus",https://www.linkedin.com/jobs/view/4251730580/?eBP=BUDGET_EXHAUSTED_JOB&refId=4V11uLfNv5ej6ik8a3vh%2BA%3D%3D&trackingId=SQ9q3Va0QQ6k5m58KVMmsw%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:11:32,Data Scientist,0,4255182864,Data Visualization Lead,Software Technology Inc.,"Pennsylvania, Estados Unidos",false,hace 7 horas,76,,,,,,"Acerca del empleo Must have Skills: Skill 1 – 7Yrs of Exp – Deck.gl, React, Skill 2 – 7Yrs of Exp – JavaScript, HTML/CSS, Skill 3 – 5Yrs of Exp – SQL  Good To have Skills: Skill 1 – 3 Yrs of Exp – Carto, Skill 2 – 3 Yrs of Exp – JSON, Skill 3 – 3 Yrs of Exp – API, Skill 4 – 3 Yrs of Exp - CI/CD, Java   Domain- Telecom is More Plus  Over View of the job: Mandatory Skills: Deck.gl, React, JavaScript, HTML/CSS, SQL Optional Skills: Carto, JSON, API, CI/CD, Java   Job Description:- We are seeking an experienced Data Visualization Lead to design and build advanced, interactive visualizations that translate complex datasets into compelling insights. This role demands a deep understanding of React and Deck.gl, along with strong UI/UX sensibilities and a collaborative, leadership-oriented mindset. You'll lead a small team of developers and work closely with data scientists, engineers, and product managers to shape data-driven user experiences.  Key Responsibilities:- Design, architect, and develop high-performance, interactive data visualizations using React, Deck.gl, and supporting technologies Lead a team of visualization engineers, providing technical guidance, code reviews, and mentorship. Collaborate with cross-functional teams to gather requirements and translate them into effective visual storytelling. Optimize rendering performance for large-scale geospatial and temporal datasets. Ensure visualizations are accessible, intuitive, and responsive across devices. Maintain a scalable visualization codebase, implementing best practices in testing, performance, and maintainability. Stay current on trends in data visualization and front-end technologies to guide the team's innovation.  Required Qualifications:- 8+ years of experience in front-end development, with at least 2 years focused on data visualization. Proficiency in React.js and Deck.gl (including Layers, Views, and integration with Carto). Strong understanding of JavaScript (ES6+), HTML/CSS, and rendering pipelines (WebGL, Canvas). Experience with data manipulation libraries (e.g., D3.js, Lodash) and working with APIs or large datasets. Familiarity with geospatial data formats (GeoJSON, TopoJSON, tilesets) and coordinate systems. Experience leading engineering projects or small teams. Excellent communication and visual storytelling skills.  Preferred Qualifications:- • Experience with design systems, Figma or Storybook. • Background in geographic information systems (GIS) or urban data. • Familiarity with state management (e.g., Redux, Zustand) and build tools (Vite, Webpack). • Knowledge of performance profiling and optimization tools. • Prior experience in fast-paced agile environment.",https://www.linkedin.com/jobs/view/4255182864/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=4V11uLfNv5ej6ik8a3vh%2BA%3D%3D&trackingId=Ns6Qcivy7Nkc7lv9%2FlTGVg%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:11:48,Data Scientist,0,4255194624,Data Scientist,micro1,Brasil,false,hace 5 horas,,,,,,,"Acerca del empleo Job Title: Data Scientist  Job Type: Full-time, Contractor  Location: Remote  About Us: Our mission at micro1 is to match the most talented people in the world with their dream jobs. If you are looking to be at the forefront of AI innovation and work with some of the fastest-growing companies in Silicon Valley, we invite you to apply for a role. By joining the micro1 community, your resume will become visible to top industry leaders, unlocking access to the best career opportunities on the market.  Job Summary: Join our customer’s team as an Data Scientist and work at the forefront of machine learning and generative AI. You will design, build, and deploy innovative AI solutions that leverage state-of-the-art technologies such as LLMs and advanced NLP, while collaborating closely with cross-functional teams. This is a unique opportunity to deliver impactful solutions on massive datasets within a high-caliber, remote-first environment.  Key Responsibilities: Build, refine, and use advanced ML engineering platforms and reusable components to deliver scalable AI solutions. Implement ML Ops processes, tracking model KPIs, monitoring drift, and establishing robust feedback loops for continuous improvement. Deploy and operationalize deep learning models, focusing on LLMs and generative AI, ensuring reliability and performance at scale. Design and orchestrate model pipelines including feature engineering, inferencing, and continuous training to meet strict SLAs. Collaborate with client-facing teams to understand high-level business context and translate it into technical requirements. Write production-ready code with a focus on testability, maintainability, and handling edge cases and errors gracefully. Participate actively in agile ceremonies, communicate progress effectively, and adhere to best practices for architecture, design, and code quality.  Required Skills and Qualifications: Bachelor’s or Master’s degree in Computer Science or a related field from a top-tier university. 4+ years of hands-on experience in machine learning, deep learning, and fine-tuning models (LLMs). Expert-level proficiency in Python; experience with backend API design and vector databases. Solid understanding of ML Ops, including measuring and tracking model performance, and MLFlow. Demonstrated experience in NLP, generative AI, and deploying real-time model predictions. Strong communication skills—both written and verbal—are essential for cross-functional collaboration. Experience with ML frameworks such as Keras and HuggingFace.  Preferred Qualifications: Familiarity with DevOps practices, CI/CD pipelines, cloud architecture, and data security. Experience in data engineering for big data systems and knowledge of PySpark or Scala. Background in computer vision and implementing end-to-end feature engineering pipelines.",https://www.linkedin.com/jobs/view/4255194624/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=4V11uLfNv5ej6ik8a3vh%2BA%3D%3D&trackingId=XYN%2BQgwzfcSg%2BLelUE3rDw%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:13:19,Data Engineer,0,4253324763,Cloud Data Engineer,The Brixton Group,Estados Unidos,false,hace 12 minutos,35,,,,,,"Acerca del empleo Location: 100% remote (must be willing to work EST or CST hours)   *** U.S. Citizens and those authorized to work in the U.S. are encouraged to apply. We are unable to sponsor at this time. ***  Our client is modernizing its data environment by transitioning from an on-premise SQL infrastructure to a scalable Snowflake cloud platform.   We're seeking a hands-on Cloud Data Engineer to support this initiative by developing proof of concepts (POCs), building data pipelines, and enabling advanced analytics for high-visibility business use cases.  Responsibilities: Design and build APIs and ETL processes to ingest and transform data from SQL into Snowflake Develop and operationalize data pipelines and organize datasets within the Snowflake environment Support data governance and ensure data integrity and availability for analytics teams Collaborate with the Snowflake professional services team and internal stakeholders Enable business outcomes by building use cases that demonstrate value to executive leadership  Requirements: Proven experience migrating data from on-prem SQL systems to Snowflake Strong knowledge of API development and ETL pipeline creation Hands-on experience with cloud data architecture and Snowflake platform Excellent communication and collaboration skills, with the ability to gather requirements and provide updates across teams  Nice to Have: Exposure to data science workflows and enabling analytics teams",https://www.linkedin.com/jobs/view/4253324763/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Bg0ErupCq9g0G1B89Dz6rA%3D%3D&trackingId=vv6ODjqGJt0OcQC1YFcSrw%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:13:35,Data Engineer,0,4245379037,Data Engineer,Jack,Estados Unidos,false,Publicado de nuevo hace 8 horas,,,,,,,"Acerca del empleo Jack is helping a leading kidney care management organization find a skilled Data Engineer to support their mission to reimagine healthcare. This company partners closely with physicians to improve the lives of patients through advanced data systems and compassionate innovation. As a Data Engineer, you'll play a critical role in building and optimizing data pipelines, supporting cloud migration efforts, and contributing to the overall architecture of a modern, scalable data ecosystem. This is an opportunity to join a fast-growing, mission-driven team focused on transforming how healthcare data is leveraged to improve outcomes. You'll collaborate cross-functionally, innovate constantly, and directly impact systems that support patients nationwide.  Responsibilities Collaborate with development teams to ensure timely and successful project delivery. Design, review, and deploy SQL code, while assisting with schema design and query optimization. Monitor, maintain, and troubleshoot data pipelines to resolve bottlenecks and ensure performance. Provide mentorship to junior engineers and guide best practices in development methodologies and frameworks. Contribute to the design and development of solutions within the existing Datawarehouse and Lakehouse architecture. Ensure proper documentation and knowledge sharing within the team to support smooth project hand-offs and scalability. Participate in defining methodologies and building tools within the Lakehouse and Datawarehouse infrastructure. Research and propose new technologies and enhancements for ongoing process improvement. Support cloud migration initiatives and the integration of data systems with external applications.  Requirements:  Education & Experience: Bachelor’s degree with 3–5 years of experience in data engineering, or Master’s degree with 3+ years of experience. Technical Expertise: Strong command of Azure-based tools including Azure Data Factory, Azure SQL databases, and Databricks. Deep experience with the Microsoft SQL Server stack (SSIS, ADF). Proficient in Python for data manipulation and pipeline support. Architecture & Integration: Understanding of data lake and Lakehouse principles, data modeling, and data warehousing best practices. Experience with cloud migration projects and integrating with third-party applications. Communication & Collaboration: Excellent verbal and written communication skills. Strong presentation skills for stakeholder meetings, requirement gathering, and feedback sessions. Work Ethic: Proven ability to meet tight deadlines without sacrificing quality. Self-driven and collaborative, with a continuous improvement mindset.   Benefits Meaningful Work: Contribute to improving the lives of patients through advanced data solutions in the healthcare space. Growth-Oriented Culture: Collaborate with experienced professionals and receive mentorship opportunities. Modern Tech Stack: Work hands-on with tools like Azure Data Factory, Databricks, SQL Server, and Python in a cloud-native environment. Innovation-First Environment: Drive improvements and suggest enhancements that shape the future of the company’s data infrastructure. Remote Flexibility: Work remotely while contributing to projects that make a real-world impact.",https://www.linkedin.com/jobs/view/4245379037/?eBP=BUDGET_EXHAUSTED_JOB&refId=Bg0ErupCq9g0G1B89Dz6rA%3D%3D&trackingId=FvRMHvrtvHhLw3Vjc6P5sw%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:13:53,Data Engineer,0,4255129998,Data Engineer,Infoplus Technologies UK Limited,"París, Isla de Francia, Francia",false,hace 11 horas,94,,,,,,"Acerca del empleo The Data Engineer designs and maintains scalable data pipelines and infrastructure for real-time analytics, ensuring data quality and security. The Data Engineer will collaborate with cross-functional teams, optimize performance, and innovate through automation.  Roles and Responsibilities: Build and Optimize Data Pipelines: Analyze and organize raw data from various internal and external sources. Design, develop, and manage scalable, high-performance and resilient data pipelines that process, transform, and make data accessible for real-time analytics and reporting. Data Infrastructure Development: Design, build, and maintain data infrastructure to ensure data is reliable, accessible, and secure for various applications and stakeholders. Implement lake-house architecture. Data Integration: Analyze business use case, perform data profiling and implement data transformations required for curating raw data into AI/ML and BI ready data. Streaming & Batch Processing: Build streaming and batch data pipelines that are reusable, fault-tolerant and scalable. Data Quality & Governance: Implement data integrity validations and data quality checks for ensuring accuracy, consistency, and completeness across all datasets. Collaboration & Cross-Functional Partnerships: Collaborate with platform engineers, data analysts, platform engineers and business stakeholders to define and implement data requirements and deliver end-to-end solutions. Data Warehouse Design: Design, implement, and optimize data lake, data warehouse and data mart to meet analytics and reporting needs.  Innovation & Automation: Identify opportunities to automate data workflows, streamline processes, and introduce best-in-class tools and frameworks that enhance productivity and efficiency. Performance Tuning: Continuously monitor and tune data pipelines and infrastructure for optimal performance, scalability, and cost-efficiency.  Required Skills: Experience: 5 to 10 years of experience building and maintaining enterprise-level data applications on distributed systems at scale. Programming Skills: Expert proficiency in SQL, Python or similar language for data processing and automation. Experience building semantic processes for distributed data applications. Data Pipeline: Experience in building scalable and resilient ELT data pipelines using modern ELT tools like DBT, Azure, Airflow, Fivetran or similar. Knowledge of streaming platforms like Kafka, Spark or similar. Data Lake & Warehouse: Experience with cloud-based data platforms like Snowflake. Advanced proficiency with SQL, writing complex queries and optimizing performance. Familiar with dimensional and party data models. Data Governance: Knowledge of data governance practices, including RBAC, RLS, data masking, data lineage and compliance with regulatory standards (e.g., GDPR, HIPAA). Experience with data governance tools such as Datahub or Collibra. Automation & DevOps: Expertise in CI/CD pipelines with tools such as GitHub Actions, Jenkins, or AWS CodePipeline and containerization (Docker, Kubernetes) to automate and manage data infrastructure. Experience in implementing observability using tools such as Prometheus, Grafana, and AWS CloudWatch. Automated testing frameworks (e.g., JUnit, PyTest).",https://www.linkedin.com/jobs/view/4255129998/?eBP=BUDGET_EXHAUSTED_JOB&refId=Bg0ErupCq9g0G1B89Dz6rA%3D%3D&trackingId=xd6IOHneAFaIA2Zy96ZCaQ%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:14:10,Data Engineer,0,4252655285,Senior Data Engineer,Lumina Tech Group,Sudáfrica,false,hace 14 horas,94,,,,,,"Acerca del empleo Senior Data Engineer - Fully Remote (South Africa)  Location: Fully Remote (South Africa) Employment Type: Full-time Contract/Permanent  About Lumina Tech Group  Lumina Tech Group is a leading technology consulting and recruitment firm with offices in the UK and UAE, and our new entity LUMINA AFRICA (PTY) LTD supporting the South African and African markets. We specialize in bridging the global tech talent gap, delivering innovative solutions in Data & AI, Cyber Security, Cloud & DevOps, and Software Development.  The Role  We're seeking a Senior Data Engineer to join our growing team and help drive data-driven innovation for our clients. This is a fully remote position based in South Africa, offering the opportunity to work with cutting-edge technologies and collaborate with international teams across different time zones. As a Senior Data Engineer, you'll be responsible for designing, building, and maintaining robust data pipelines and infrastructure that enable our clients to unlock the full potential of their data assets.  Key Responsibilities  • Design and implement scalable data pipelines and ETL/ELT processes • Build and maintain data warehouses and data lakes using modern cloud platforms • Develop real-time and batch data processing solutions • Collaborate with data scientists and analysts to ensure data accessibility and quality • Optimize data storage and retrieval systems for performance and cost-efficiency • Implement data governance and security best practices • Monitor and troubleshoot data infrastructure and resolve issues proactively • Mentor junior team members and contribute to technical documentation • Work closely with cross-functional teams to understand business requirements  Required Skills & Experience  Technical Skills: • 5+ years of experience in data engineering or related field • Strong proficiency in Python, SQL, and at least one other programming language (Java, Scala, or Go) • Experience with cloud platforms (AWS, Azure, or GCP) and their data services • Hands-on experience with big data technologies (Spark, Hadoop, Kafka, etc.) • Knowledge of data warehousing concepts and tools (Snowflake, Redshift, BigQuery) • Experience with containerization (Docker, Kubernetes) and CI/CD pipelines • Familiarity with data modeling and database design principles • Understanding of data governance, security, and compliance requirements  Soft Skills: • Strong problem-solving and analytical thinking abilities • Excellent communication skills and ability to work in remote, distributed teams • Self-motivated with strong time management skills • Collaborative mindset with a passion for continuous learning • Attention to detail and commitment to delivering high-quality solutions  Preferred Qualifications  • Bachelor's degree in Computer Science, Engineering, or related field • Experience with machine learning pipelines and MLOps • Knowledge of streaming data technologies (Apache Kafka, Kinesis) • Familiarity with Infrastructure as Code (Terraform, CloudFormation) • Previous experience working with international clients or distributed teams • Relevant certifications in cloud platforms or data technologies",https://www.linkedin.com/jobs/view/4252655285/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Bg0ErupCq9g0G1B89Dz6rA%3D%3D&trackingId=JI4ruq1RBYDrOPnTbM1JMw%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:14:29,Data Engineer,0,4255235099,AWS Data Engineer,Jobs via Dice,Estados Unidos,false,hace 3 horas,,,,,,,"Acerca del empleo Dice is the leading career destination for tech experts at every stage of their careers. Our client, Devfi, is seeking the following. Apply via Dice today!  Job Title: Data Engineer  Duration: 6+ Months  Exp: 12+ Years  Pay Rate: $50/hr  W2 Only!!  Remote!!   Job Description:  A tech engineer with Python/AWS experience who can design payloads for processing to Grid. Perform Data Mapping, Data Transformations and then build prototypes for Payload processing. The actual implementation and development will be done by the IT of the client. Project is related to Futures/Debt / Derivatives trading analytics Data will be in Numerix (Financial Asset / risk management) platform system - extract and send the data to the grid (where we run massive models for calculating positions etc.) They re likely to use either proto buffer or Avro for data payloads. Python/AWS Someone with 5 to 7 years experience with strong communication skills Hands on coding and development experience The financial services industry is a must - within finance preferred loan, structured assets, mortgage back securities data familiarity - understand the terminologies like Swap rate, maturity rate, date , index, days of conventions (business days, calendar days), instrument type etc. for data mapping and data transformations  Thanks,  Naveen S",https://www.linkedin.com/jobs/view/4255235099/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Bg0ErupCq9g0G1B89Dz6rA%3D%3D&trackingId=KZi0QUVzHbn1FUdAHkLXqg%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:14:46,Data Engineer,0,4206596879,Azure Data Engineer,Cloudforce,"National Harbor, MD",false,Publicado de nuevo hace 9 horas,100,,,,,,"Acerca del empleo Job Title: Azure/Fabric Engineer  Type: Full Time  Compensation: $80,000 to $165,000+ DOE, Annually + Bonus Opportunities  Please note - applicants must be authorized to work for any employer in the U.S. We are unable to sponsor or take over sponsorship of an employment visa at this time.  Gnarly tech problems, monthly team outings, Friday shenanigans, and the Microsoft cloud…. Want in? You’ve come to the right place.  Cloudforce is seeking a talented, full-time Fabric Engineer, with experience designing and implementing solutions in Microsoft Azure and Fabric. This role requires hands-on engineering of cloud infrastructure, data, and AI solutions to meet the complex requirements of various high-profile customers.  Responsibilities  Work closely with clients to understand their current infrastructure, requirements, and business goals.  Design, develop, and migrate scalable data pipelines and ETL processes on Microsoft Fabric. Design and implement data integration processes to extract, transform, and load (ETL) data from various sources such as on-prem, external 3rd party applications, and other cloud providers. Create data processing workflows and pipelines to support data analytics, machine learning, and other analytic-driven applications. Implement CI/CD pipelines for automating data engineering processes using Azure DevOps or GitHub. Collaborate with cross-functional teams to understand data requirements and implement effective data integration solutions. Set up and configure various Microsoft security solutions such as Purview and Sentinel to monitor and alert security and compliance best practices throughout the data & analytics lifecycle. Set up monitoring solutions using Azure Monitor, Azure Log Analytics, Fabric Data Activator, and other Azure solutions to track the performance of data pipelines or analytics workflows to ensure data quality, integrity, and performance. Conduct thorough testing and validation of data solutions to ensure the accuracy, completeness, and reliability of the delivered data products. Implement, optimize, and maintain cost-effective data & analytics solutions by leveraging Azure Cost Management tools and best practices. Stay updated on industry regulations and implement necessary changes to maintain compliance. Other duties, as assigned.  Qualifications  3+ years of experience with cloud engineering, particularly in Azure and/or Microsoft Fabric. Proficiency in Azure cloud services, including Azure Data Factory, Azure Databricks, Azure SQL Database, Azure Synapse Analytics, and Azure Blob Storage. Experience with Microsoft SQL Server tools (SSIS, SSRS, and SSAS). Experience with data engineering processes using Apache Spark, Python, R, T-SQL, or KQL. Experience with generating Semantic Models. Familiarity with Microsoft Power BI for data visualization. Passion for staying updated on industry trends and emerging data engineering and cloud computing technologies. Strong communication skills and the ability to collaborate effectively with cross-functional teams.  Preferred Qualifications  Microsoft Azure certifications (e.g., Azure Fabric Data Engineer Associate, Azure Solutions Architect Expert). Experience with big data technologies such as Hadoop, Snowflake, or DataLakes. Experience with real-time analytics technologies such as Azure IoT Hub, Fabric Real-Time Analytics, or Kafka. Bachelor's degree in Computer Science, Data Engineering, or related field.  You Love To  Work in a dynamic team environment.  Learn and deploy modern technologies.  Perform as a self-starter and manage your own time.  Analyze and solve tough technical problems by leveraging leading-edge technologies.  Demonstrate your expertise through a consultative and collaborative approach to engineering.  Interact with clients often and maintain excellent working relationships.  Participate in monthly company outings and quarterly local service projects.  Eat lunch as a team every Friday and have your hand at conquering our ping-pong champions.   About Us:  Cloudforce is a spirited team defined by the shared values of excellence, growth, teamwork, passion, giving back, and glee. As technophiles, we thrive on the latest developments in our chosen field of expertise: cloud computing. As humans, we are driven by the opportunities to make life better through the thoughtful application of technology. At Cloudforce, these two pursuits combine to form an effective, human-centered approach for making cloud solutions accessible for businesses, app developers, and entrepreneurs, alike.  We offer our employees unique opportunities to learn, grow, and be part of a team that believes in more than just typical nine-to-five activities. We’ve built a culture around openness, inclusiveness, giving back to the community, team-building, and growth. Whether it be through monthly team outings, annual trips, or our frequent charitable activities, we’re serious about making each team member feel like they’re part of our team.  Cloudforce offers everything you’d expect in the perfect technology job…   Outstanding opportunities to learn, grow, and expand your network.  Excellent compensation, benefits, and generous incentives.  Complimentary snacks to keep you focused.  Super cutting-edge technology.  State-of-the-art workspace.  Community involvement.  Great team synergy.  But we also offer a few irresistible extras:   Friday lunch and shenanigans... on us!  Incentive program for investing in your growth.  401K savings plan and education reimbursement.  24/7 access to a modern gym with Tonal and Peloton.  Free monthly garage parking with direct private access to the office.  Brand-new, sun-filled National Harbor offices with scenic views of the Potomac, surrounded by shops, restaurants, and more.  P.S.... Wondering about our other essential benefits? Here’s a brief snapshot:   Medical, dental, life, and short-term disability insurance covered at 100% of the premium for employees and 50% for dependents.  Paid parental leave, including adoption and foster care placement.  PTO starting at 15 days during your first two years of employment, 20 days in years 2 through 4, and 25 days after 4 years (+ incentives opportunities to earn more PTO!).  9 company-observed holidays + 2 more floating holidays to cover additional observed holidays or for use as extra PTO.  And more! Check out our careers page for more details: www.gocloudforce.com/careers/.  Cloudforce is an Equal Opportunity/Affirmative Action employer. All qualified candidates will receive consideration for employment without regard to disability, protected veteran status, race, color, religious creed, national origin, citizenship, marital status, sex, sexual orientation/gender identity, age, or genetic information.  Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions in accordance with the Americans with Disabilities Act.",https://www.linkedin.com/jobs/view/4206596879/?eBP=BUDGET_EXHAUSTED_JOB&refId=Bg0ErupCq9g0G1B89Dz6rA%3D%3D&trackingId=G71jGkHn09jLYVUD3kG9tg%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:15:04,Data Engineer,0,4252679892,Big Data Engineer,Kresta Softech Private Limited,India,false,hace 8 horas,100,,,,,,"Acerca del empleo We’re looking for a skilled Big Data Engineer.  Role Highlights: Position: Big Data Engineer Experience: 4+ years Location- Remote Work mode- WFH Notice Period: Immediate/15 days joiners Mandatory.  Key Skills:-  Big Data, AWS, CI/CD Pipelines, Scala, Python or Java or C++",https://www.linkedin.com/jobs/view/4252679892/?eBP=BUDGET_EXHAUSTED_JOB&refId=Bg0ErupCq9g0G1B89Dz6rA%3D%3D&trackingId=50yf46q9jr1IxE0PZSAJFQ%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:15:22,Data Engineer,0,4252696254,Data Engineer - AI,RED Global,India,false,hace 6 horas,100,,,,,,"Acerca del empleo Data Engineer - Remote - 6 months contract, extendable - Pharma  We are seeking a highly skilled Data Engineer to design, build, and maintain scalable data pipelines using Snowflake and Databricks. This role is ideal for someone passionate about transforming complex data into actionable insights and supporting cutting-edge AI initiatives, including NLP and Generative AI applications.  Key Responsibilities:  Design, implement, and maintain robust and scalable data pipelines using Snowflake and Databricks Perform advanced data wrangling, cleansing, and transformation to support analytics and machine learning workflows Collaborate closely with data scientists and machine learning engineers to enable NLP-based customer journey modeling Explore, evaluate, and integrate Generative AI, LLMs, and vector databases into data and AI pipelines Ensure data quality, reliability, and performance across all data layers and systems  Required Qualifications:  Proven experience building and maintaining data pipelines with Snowflake and Databricks Strong proficiency in SQL and Python Familiarity with Generative AI, Large Language Models (LLMs), vector databases, or NLP techniques Strong problem-solving skills and the ability to work independently and collaboratively",https://www.linkedin.com/jobs/view/4252696254/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Bg0ErupCq9g0G1B89Dz6rA%3D%3D&trackingId=2%2Ba%2BjN5ntiyWE3GAgQdAJA%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:15:38,Data Engineer,0,4255204410,"Data Engineer, Entry Level",Jobright.ai,"Newport Beach, CA",false,hace 7 horas,,,,,,,"Acerca del empleo At Jobright, we help high-growth startups hire top talent for their key roles.  About MIG Real Estate MIG Real Estate (“MIG”) is a private real estate investment company headquartered in Newport Beach, California, with over $3.4 billion of real estate investments in multifamily, industrial, office, hotel and retail properties located primarily in the Western U.S. and Sun Belt. The Company is dedicated to acquiring and repositioning real estate in a way that transforms communities, creatively enhances quality of life and dramatically improves the environments where our customers live, work and play. The company is focused on significant growth through acquisitions, redevelopment and repositioning of assets, continuously creating value with long-term stability throughout all market cycles.  Role Snapshot As a Data Engineer on our team, you will be instrumental in building scalable data pipelines, modeling clean and reliable datasets, and helping ensure that MIG’s analytics infrastructure continues to evolve alongside our growing business. This role requires a strong technical foundation, attention to detail, and excellent interpersonal skills to collaborate across technical and non-technical teams.  Key Responsibilities Ingest data from multiple structured and unstructured sources (e.g., APIs, flat files, cloud services, etc.). Implement pipeline orchestration, scheduling, and monitoring to ensure timely data delivery. Develop and maintain well-structured data models that serve reporting and analytics needs. Collaborate with technology team and business users to translate reporting requirements into dimensional models. Write complex SQL queries, stored procedures, and CTEs to perform robust data transformations. Develop reusable SQL patterns and documentation to promote best practices within the team. Perform root cause analysis and data lineage tracing for issues identified by users or systems. Contribute to the documentation of data dictionaries, business logic, and metadata. Manage compute and storage resources to balance cost and performance.  Must-Have Qualifications Bachelor’s degree or equivalent hands-on experience. 1-3 years’ experience in a data engineering type role, minimum. Demonstrated ability to work closely with users and stakeholders to translate business needs into technical solutions. Experience in Python for API and pipeline scripting. Advanced in SQL, including performance tuning and query optimization. Strong data modeling skills. Hands-on experience with cloud data tools such as Azure Synapse / Data Factory, Databricks, or comparable platforms. Strong project execution abilities and a track record of cross-functional collaboration. Willing & able to travel domestically 15-20% of the year.  Preferred Qualifications Experience in the real estate or property asset management industry. Exposure to Power BI, DAX, or Microsoft Fabric is a plus. Familiarity with Git-based version control and CI/CD practices.  Why Join MIG? Join a values-driven, people-first organization that fosters professional development and encourages autonomy. Make an immediate impact on real estate investment strategy through high-visibility data initiatives. Work with a modern cloud-based data stack in a growth-oriented environment. Competitive compensation, performance-based bonuses, comprehensive benefits, and generous PTO.",https://www.linkedin.com/jobs/view/4255204410/?eBP=BUDGET_EXHAUSTED_JOB&refId=Bg0ErupCq9g0G1B89Dz6rA%3D%3D&trackingId=xyvAa7YIkcqhsbKwXg8LUw%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:15:54,Data Engineer,0,4255131804,Data Science & ML Engineer,TrueNorth®,Reino Unido,false,hace 11 horas,100,,,,,,"Acerca del empleo One of our clients are a leading E-commerce company are looking for an DS/ML engineer to join them for an initial 6 months contract.  Details of this contract as follows:  Location: Remote Duration: 6 months, high likelihood of extension Rate: £400 - £450 per day Status: Outside IR35 Start date: Ideally immediate (they may be able to wait up to 4 weeks for someone to start) Skills required: NLP, Modelling, Python, SQL, Databricks, Anomaly Detection Desirable: Image Classification, Airflow, Pyspark   You will be joining their customer security team, which uses machine learning to detect anomalies and erroneous activity on customer accounts. You will be building models to help detect any unwanted account takeovers by cyber criminals and build customer trust.  If this is of interest, then please apply today or send through an updated CV to:  david@truenorthgroup.io",https://www.linkedin.com/jobs/view/4255131804/?eBP=BUDGET_EXHAUSTED_JOB&refId=Bg0ErupCq9g0G1B89Dz6rA%3D%3D&trackingId=xK8Y1bmZ19a0yod4Q7dEOg%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:16:10,Data Engineer,0,4255214202,Senior Data Engineer,Harnham,"Inglaterra, Reino Unido",false,hace 6 horas,42,,,,,,"Acerca del empleo DATA ENGINEER - DBT / AIRFLOW / DATABRICKS  4-MONTH CONTRACT   £450-550 PER DAY OUTSIDE IR35 This is an exciting opportunity for a Data Engineer to join a leading media organisation working at the forefront of data innovation. You'll play a key role in designing and building the data infrastructure that supports cutting-edge machine learning and LLM initiatives. Expect to work closely with data scientists, helping to power intelligent content and audience tools across the business. THE COMPANY  A major player in the media sector, this company is investing heavily in its data capabilities to support everything from real-time analytics to AI-powered content discovery. The environment is collaborative, fast-moving, and focused on innovation. With strong engineering foundations already in place, they're looking for a contractor to accelerate delivery of critical pipelines and platform improvements. THE ROLE  You'll join a skilled data team to lead the build and optimisation of scalable pipelines using DBT, Airflow, and Databricks. Working alongside data scientists and ML engineers, you'll support everything from raw ingestion to curated layers powering LLMs and advanced analytics.  Your responsibilities will include: Building and maintaining production-grade ETL/ELT workflows with DBT and Airflow Collaborating with AI/ML teams to support data readiness for experimentation and inference Writing clean, modular SQL and Python code for use in Databricks Contributing to architectural decisions around pipeline scalability and performance Supporting the integration of diverse data sources into the platform Ensuring data quality, observability, and cost-efficiency KEY SKILLS AND REQUIREMENTS Strong experience with DBT, Airflow, and Databricks Advanced SQL and solid Python scripting skills Solid understanding of modern data engineering best practices Ability to work independently and communicate with technical and non-technical stakeholders Experience in fast-paced, data-driven environments DESIRABLE SKILLS Exposure to LLM workflows or vector databases Experience in the media, content, or publishing industries Familiarity with cloud data platforms (e.g., AWS or Azure) Knowledge of MLOps and ML/data science pipelines HOW TO APPLY  To register your interest, please apply via the link or get in touch with your CV.",https://www.linkedin.com/jobs/view/4255214202/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Bg0ErupCq9g0G1B89Dz6rA%3D%3D&trackingId=%2FXNZ8S46ZkWjpECkZB15YQ%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:16:27,Data Engineer,0,4249992260,Senior Data Engineer,Zonda,Canadá,false,Publicado de nuevo hace 3 horas,,,,,,,"Acerca del empleo Senior Data Engineer   Remote Canada | Full Time  Salary Range: $120,000 - $150,000 CAD annually  Zonda is redefining the future of housing. We are perfectly placed in the heart of the fast-growing real estate industry. We are making big bets on the future of  real-estate , trailblazing a 2030 vision for the industry. Here at Zonda, you’ll be able to use your passion and curiosity to drive the next generation of real estate analysts, advisors, and technologists.  Zonda is looking for a passionate Senior Data Engineer to evolve and expand our team. Zonda looks for people who can grow, think, dream, and create. When you join our team, you’ll be in a unique position to make a positive impact with every project. You’ll use your full range of skills to build great experiences and learn about the real estate industry, economics, and data. You’ll be supported with the necessary tools, and you'll be working with an awesome and like-minded team. Our teams are innovative, diverse, multidisciplinary, and collaborative - all working to build the future of housing.   As a Senior Data Engineer, you will play a crucial role in designing, implementing, and optimizing our data infrastructure. You will be embedded in an application scrum team working on a large-scale application. Your expertise in MSSQL Server query optimization, Snowflake, and SSIS/Glue ETL performance tuning will be instrumental in ensuring the efficiency and reliability of our data systems.   What You’ll Do  Data Pipeline Processes:    Design, implement, and maintain data pipeline processes for efficient data extraction, transformation, and loading.   Collaborate with cross-functional teams to integrate data from various sources into target databases.    Database Design and Development:    Design and implement robust, scalable, and performance-optimized database solutions using MSSQL Server and Snowflake.   Develop and maintain database procedures, schemas, tables, indexes, and views to support application requirements.    Query Optimization:    Analyze and optimize SQL queries for maximum efficiency and performance.   Identify and resolve performance bottlenecks in database systems through the use of query plans.   Collaborate with application developers to enhance query performance and database responsiveness.    Data Modeling:    Work closely with data engineering to design and implement data models that align with business requirements.   Ensure data integrity and consistency across the database environment.    Monitoring and Maintenance:    Implement effective monitoring solutions to proactively identify and address potential issues.   Perform routine maintenance tasks such as backups, restores, data purges, and index de-fragmentation.   Experience with Unix and  Powershell  scripting technologies    Collaboration and Documentation:    Collaborate with cross-functional teams, including data scientists, product analysts, and application developers, to understand data requirements.   Participate in architectural discussions and technical decision-making processes.   Effectively communicate technical concepts to non-technical stakeholders.   Document database design, configurations, and procedures.    Who You Are   Bachelor's degree in Computer Science , Information Technology, or a related field.   Proven experience as a Data Engineer with a focus on building efficient data pipelines using SSIS/Glue and Data Warehouses using Snowflake.   Experience with ETL tools and processes ( eg.  Airflow, AWS Glue/Spark, SSIS, DBT).   Experience with cloud technologies (S3, EC2, Linux)   In-depth knowledge of database design, query optimization, and performance tuning.   Proficiency in writing complex SQL queries and stored procedures in MSSQL Server and Postgres databases.   Experience with database scalability and writing deep queries accessing large datasets   Experience with GitHub and CICD systems   Experience with database security best practices.   Experience with modern big data frameworks.   Experience with Python.   Excellent problem-solving and analytical skills.   Effective and proactive communication skills and ability to work collaboratively in a team environment.    Preferred Skills:    Knowledge of cloud-based database solutions (e.g., Amazon RDS (SQL Server), Azure SQL Database).   Experience with Apache Airflow or Astronomer   Experience with other database technologies (PostgreSQL,  CitusData ).   Experience building data pipelines that utilize Kinesis and Lambda functions   Certification in Data Engineering or related technologies.   Experience with Geo Spatial data and queries is a plus    Why People Love Working Here   We offer meaningful work and opportunities for career growth   Competitive Salary   100% paid by Zonda health benefit package   100% Company paid Life Insurance coverage   Paid vacation and general holidays   Employee Assistance Program (EAP)   Live Meditation Sessions   Employee Recognition Platform   Virtual Wellness Program   100% Hybrid – and always will be!   Visionary Leadership Team    Inclusion & Equal Opportunity Employment  Zonda (formerly Hanley Wood | Meyers Research) is proud to be an Equal Opportunity Employer committed to diversity, inclusion & belonging. Here at Zonda, we are interested in every qualified candidate who is eligible to work in Canada.",https://www.linkedin.com/jobs/view/4249992260/?eBP=BUDGET_EXHAUSTED_JOB&refId=Bg0ErupCq9g0G1B89Dz6rA%3D%3D&trackingId=VwJ%2FXrXK%2FvBv9%2BrWXQb9UA%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:16:43,Data Engineer,0,4255129881,Data Engineer,,"Europa, Oriente Medio y África",false,hace 12 horas,,,,,,,"Acerca del empleo As a trusted recruitment partner, committed to connecting skilled professionals with leading organizations worldwide. We are currently hiring on behalf of a cutting-edge software company seeking an experienced Data Engineer to design, build, and manage scalable data pipelines and infrastructure. This is a full-time, remote opportunity open to professionals located within the EMEA region. About the Role As a Data Engineer, you will play a key role in the development and optimization of data architecture and workflows. You’ll work with large-scale datasets and collaborate with cross-functional teams to ensure the availability, performance, and scalability of data systems that power business intelligence and analytics solutions. Key Responsibilities Design, build, and maintain scalable data pipelines and infrastructure for large datasets. Develop ETL processes to collect, transform, and store data from various sources. Optimize data systems for performance, scalability, and reliability. Collaborate with data scientists, analysts, and developers to support data-driven initiatives. Ensure data quality, integrity, and governance across platforms. Monitor and troubleshoot data flow issues, and resolve data-related technical problems. Evaluate and implement emerging technologies for data engineering improvements. Qualifications Proven experience as a Data Engineer, working with large-scale data architecture. Proficiency in SQL and data pipeline tools (e.g., Apache Spark, Kafka, Airflow). Experience with cloud platforms such as AWS, GCP, or Azure. Strong programming skills in Python, Scala, or Java. Familiarity with big data tools and distributed systems (e.g., Hadoop, Hive, Redshift, BigQuery). Understanding of data warehousing concepts and best practices. Excellent problem-solving, communication, and collaboration skills. Bachelor’s or Master’s degree in Computer Science, Engineering, or a related field. What’s Offered Full-time remote opportunity within the EMEA region. Competitive salary and performance-based growth opportunities. Work on innovative data infrastructure projects with a global software leader. Collaborative and forward-thinking team culture. 📩 Apply Now! If you're passionate about building robust data systems and enabling data-driven innovation, we’d love to hear from you!",https://www.linkedin.com/jobs/view/4255129881/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Bg0ErupCq9g0G1B89Dz6rA%3D%3D&trackingId=7m7%2FJTufB0AfdRT5N8yzDg%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:17:00,Data Engineer,0,4255255035,Remote GCP Data Engineer,Insight Global,India,false,hace 1 hora,12,,,,,,"Acerca del empleo Qualifications: SQL GCP BigQuery Dataform and/or DBT Python Must be collaborative and open to learning Must be a systemic-thinker  Nice to Have: Candidates coming from a retail company who understand/have experience working on projects with customer data Machine Learning experience – “good if they can work on models or want to learn models” Experience working with really high streaming/high velocity/fancy technologies (seeing that in action at a decent-sized company is great background experience)   Day to Day: A Fortune 100 retail client of Insight Global is looking to hire elite Data Engineers to join their Enterprise Data & Analytics Team. This team is responsible for supporting and providing all 12 company business units with the data they need to optimize their BUs and increase efficiency/profitability. This company has a centralized data organization and is Google's largest customer. There are over 200 people that report to this director, and his teams are responsible for working with over 170 pedabytes of data within BigQuery. This Data Engineer will be responsible for legacy redesigns, net new projects within all 12 business units, and will be using cutting-edge technology for all of it. The ideal candidate for this role can vary in years of experience but must be able to think systemically about computer science instead of just being able to code. For example, there are a hundred ways to cut code, but these candidates must understand what the trade-offs are of doing it each way and be able to explain why they would decide which way they chose to cut it. We are looking for someone with a growth mindset who is able to reflect on their prior work and think of what they would do differently next time to improve it. The overarching goal of this team is not just build a data set, but to build one that is robust using processes and techniques that will allow to work just as well on day 100 as it would on day 1.  **Pay can vary and depends on education, skills and experience. Benefits and PTO provided as required in India",https://www.linkedin.com/jobs/view/4255255035/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Bg0ErupCq9g0G1B89Dz6rA%3D%3D&trackingId=Pke0Q2Kq1FEXA9dmE4mANA%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:17:16,Data Engineer,0,4252668416,Principal Data Engineer,Maxwell Bond,Reino Unido,false,hace 12 horas,54,,,,,,"Acerca del empleo Principal Data Engineer BI/Reporting/ Datawarehouse (BigQuery) Sports and Analytics Sector Location: Remote (UK Based Candidates Only)  Could your expertise help discover the next Lionel Messi, LeBron James or Lewis Hamilton? Maybe give your team a competitive edge to finally win the title?  We have an exceptional opportunity for a Principal Data Engineer specialising in Business Intelligence, Data, and Reporting to join a leading global data analytics company. This organization is at the forefront of using innovative software and intelligence tools to enhance athlete performance and identify the next stars in their respective sports.  As the company embarks on a significant re-architecture of its Data Warehouse, they are looking for a talented Senior Engineer to help drive this transformation.  The data and reporting platforms you develop will play a crucial role in influencing the performance and decision-making of major teams in sports such as American football, soccer, NBA, Formula 1, and Olympic events.  Key Responsibilities:  Design and build large-scale data warehouses, ETL pipelines, and reporting platforms that are robust and efficient. (They've just confirmed to be going down the Google stack with BigQuery) Utilize your expertise in backend languages; the tech stack is flexible, including Java, Python, .NET, Ruby, and more. Implement strong coding principles, including CI/CD, TDD, and maintain high-quality code standards. Bring a deep passion for data and a track record of building data warehouses from the ground up.  What’s in it for You?  Competitive salary of up to £100k Equity options Annual bonus Private healthcare Fully remote work Exciting perks, including tickets to events and team-building away days! (Yes, there is literally an option where you can get tickets for El Classico, Monza, and other events!)  This is an incredible chance for a collaborative Senior or Principal Engineer to contribute to applications that are utilised by your favourite teams and revolutionise how data is leveraged in sports. Don’t miss out on this opportunity to make a meaningful impact!  Apply Now!",https://www.linkedin.com/jobs/view/4252668416/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Bg0ErupCq9g0G1B89Dz6rA%3D%3D&trackingId=yne1%2BE%2F7qNfxzIPwGB93JQ%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:17:34,Data Engineer,0,4235435771,Data Engineer (Power BI/ADF/Power Apps) - 100% Remote,UST España & Latam,España,false,Publicado de nuevo hace 15 horas,100,,,,,,"Acerca del empleo 🚀 We are looking for the very Top Talent…and we would be delighted if you were to join our team! More in details, UST is a multinational company based in North America, certified as a Top Employer and Great Place to Work company with over 35.000 employees all over the world and presence in more than 35 countries. We are leaders on digital technology services, and we provide large-scale technologic solutions to big companies.  🔎 What we look for? We are looking for a Data Engineer. Our client is a multinational financial company.  📋 Main tasks and accountabilities will be: Power BI Development: Design, develop, and optimize interactive Power BI reports and dashboards, ensuring clear visuals and actionable insights. Collaborate with business stakeholders to ensure reports meet business needs and provide valuable, data-driven insights. Perform data transformations and modeling within Power BI using tools such as Power Query and DAX. Manage data security, role-based access, and user-level permissions within Power BI. Azure Data Factory (ADF): Develop and maintain data pipelines using Azure Data Factory (ADF) for seamless data integration, migration, and transformation.  💡 What do we expect from you? Proven experience with Power BI, Azure Data Factory (ADF), SQL Server, and integrating Power Apps into reports. Strong expertise in creating insightful, visually appealing reports and dashboards using Power BI. Basic understanding of Microsoft Fabric and its role within the Power BI ecosystem. Familiarity with other data visualization tools like Tableau is a plus. Strong SQL skills for database management, query writing, and ETL processes. Experience with cloud-based data solutions and data transformation tools, including Azure Data Factory. Excellent Communication: Ability to communicate complex technical concepts to non-technical stakeholders in a clear and actionable manner.  💰 What can we offer? ✈️ 23 days of Annual Leave plus the 24th and 31st of December as discretionary days! ❤️ Numerous benefits (Health Care Plan, teleworking compensation, Life and Accident Insurances). 🍴 `Retribución Flexible´ Program: (Meals, Kinder Garden, Transport, online English lessons, Health Care Plan…) 🎓 Free access to several training platforms 🚀 Professional stability and career plans 🚶 UST also, compensates referrals from which you could benefit when you refer professionals. 🔗 The option to pick between 12 or 14 payments along the year. 🏡 Real Work Life Balance measures (flexibility, WFH or remote work policy, compacted hours during summertime…) 🎁 UST Club Platform discounts and gym Access discounts    📧 If you would like to know more, don’t hesitate to apply and we’ll get in touch to fill you in detail. We are waiting for you! In UST we are committed to equal opportunities in our selection processes and do not discriminate based on race, gender, disability, age, religion, sexual orientation or nationality. We have a special commitment to Disability & Inclusion, so we are interested in hiring people with disability certificate.",https://www.linkedin.com/jobs/view/4235435771/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Bg0ErupCq9g0G1B89Dz6rA%3D%3D&trackingId=54SPPiSZjDUxO7Q%2ByfCixQ%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:17:50,Data Engineer,0,4255215594,"Contract role: Snowflake Data engineer at Mississauga, Canada (Remote)",Yochana,"Mississauga, ON",false,hace 5 horas,74,,,,,,"Acerca del empleo Snowflake Data engineer  Mississauga, Canada (Remote)  1 year +   Skills req: Snowflake, IICS, SQL,  Proficient in writing complex SQL queries, stored procedures, and query optimization for large-scale data processing. Hands-on experience with Snowflake including schema design, Snowpipe, Streams & Tasks, and performance tuning. Expertise in developing ETL/ELT pipelines using Informatica IICS for cloud and hybrid data integration. Strong understanding of data modeling techniques including star and snowflake schema. Experience in integrating Snowflake with cloud storage solutions like AWS S3 or Azure Blob for data ingestion. Design and develop scalable data pipelines using SQL, Snowflake, and IICS for efficient data movement and transformation. Create and manage Snowflake objects including warehouses, databases, schemas, tables, streams, and tasks. Develop ETL/ELT processes using Informatica IICS, ensuring seamless integration between cloud and on-premise data sources. Implement data models using best practices like star and snowflake schemas to support analytical reporting. Perform data validation and apply quality checks to ensure high data accuracy and consistency. Monitor job performance, optimize resource usage, and troubleshoot issues in Snowflake and IICS environments. Good communication and coordination skills",https://www.linkedin.com/jobs/view/4255215594/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Bg0ErupCq9g0G1B89Dz6rA%3D%3D&trackingId=neDKkFrwChc4nHXHi8Ywqw%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:18:08,Data Engineer,0,4255158336,REMOTE - Data Engineer,Lensa,"Woonsocket, RI",false,hace 9 horas,,,,,,,"Acerca del empleo Lensa partners with DirectEmployers to promote this job for Insight Global.  Job Description  An employer is seeking a Data Engineer to join a leading healthcare and pharmaceutical company. Responsibilities include:   Be a technologist and work with other Engineers in planning, prioritizing, and performing assigned tasks within deadlines   Will be responsible for end-to-end application development & delivery including production deployment, application operationalization, and observability.   Develop data pipelines to ingest, process and broadcast data across heterogenous databases.   Build and deploy using GitHub, CircleCI, Harness as part of CI/CD process in leading Cloud Platforms - AWS, GCP or Azure etc.   Continuously checking and monitoring App health and KPIs, support triage of any production issues as and when needed   Be an advocate for and implementer of security best practices   Adopt and apply industry technology best practices   Partner with application owners, business partners and peer groups regarding long and short-range technical solutions that meet business requirements.   Analyze and contribute to project and business requirements based on product team milestones and priority.   Actively participate in Agile Scrum team activities including Sprint Planning, Grooming, Scrum, Reviews and Retrospectives  $50-55/hour  Exact compensation may vary based on several factors, including skills, experience, and education.  Benefit packages for this role will start on the 31st day of employment and include medical, dental, and vision insurance, as well as HSA, FSA, and DCFSA account options, and 401K retirement account access with employer matching. Employees in this role are also entitled to paid sick leave and/or other paid time off as provided by applicable law.  We are a company committed to creating inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity employer that believes everyone matters. Qualified candidates will receive consideration for employment opportunities without regard to race, religion, sex, age, marital status, national origin, sexual orientation, citizenship status, disability, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to Human Resources Request Form (https://airtable.com/app21VjYyxLDIX0ez/shrOg4IQS1J6dRiMo) . The EEOC ""Know Your Rights"" Poster is available here (https://www.eeoc.gov/sites/default/files/2023-06/22-088_EEOC_KnowYourRights6.12ScreenRdr.pdf) .  To learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .  Skills And Requirements  7+ years experience in software engineering from ideation to production deployment of software   7+ years experience in full software development life cycle including ideation, coding, coding standards, testing, code reviews and production deployments   Experience in creating/managing GCP storage Buckets, Data Composer workflow, Dataflow jobs, IAM (Service Account/Roles) Management.   Experience in data extraction, transformation, loading (ETL), data quality checks, database management Experience in Writing automated unit and mock test cases for code coverage   Deployment and troubleshooting application in Cloud environment (GCP)   Experience with Kubernetes, Apigee, GCP -GKE, Dataflow, Airflow. MongoDb, SQL, Postgres SQL, SOAP services, IntelliJ and Devops: Git, Jenkins, Github Actions.   Experience in log ingestion and building dashboards using Prometheus and Grafana.   Proficiency in SQL and NoSQL database technologies   Query and decipher logging entries by application tiers and components using trace logs   Form requests in tooling like Postman , SOAP UI to invoke endpoints   Experience working in a Scrum/Agile development methodology   Ability to work independently and part of a team   Healthcare Domain experience null  We are a company committed to creating diverse and inclusive environments where people can bring their full, authentic selves to work every day. We are an equal employment opportunity/affirmative action employer that believes everyone matters. Qualified candidates will receive consideration for employment without regard to race, color, ethnicity, religion,sex (including pregnancy), sexual orientation, gender identity and expression, marital status, national origin, ancestry, genetic factors, age, disability, protected veteran status, military oruniformed service member status, or any other status or characteristic protected by applicable laws, regulations, andordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request to HR@insightglobal.com.  If you have questions about this posting, please contact support@lensa.com",https://www.linkedin.com/jobs/view/4255158336/?eBP=BUDGET_EXHAUSTED_JOB&refId=Bg0ErupCq9g0G1B89Dz6rA%3D%3D&trackingId=GGIlTAokIzisPdPjbbEUwQ%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:18:27,Data Engineer,0,4251791633,Data Engineer,AFRICAWORK,Kenia,false,hace 12 horas,100,,,,,,"Acerca del empleo We are recruiting a freelance Data Engineer on behalf of our client, a company operating in the custom software development and artificial intelligence services sector. The position is fully remote.  Missions :  Involving in the product development life cycle from brainstorming ideas to designing quality interactive data visualizations that tell a story with elegance Engaging in every step of the development life cycle, from collecting requirements and communicating to users, to designing, developing, and testing the products Working with data solutions, and data modeling, understanding ETL pipelines, and dashboard tools Interacting with other team members to develop technical specifications, including documentation to support production code Exploring and promoting strategies to improve our data modeling, quality, and architecture  Primary Technical Proficiencies: SQL Server Product Suite: Strong expertise in SQL Server Management Studio (SSMS), SQL Server Integration Services (SSIS), SQL Server Analysis Services (SSAS), and SQL Server Reporting Services (SSRS) Microsoft Azure Cloud Resources: Solid experience with Azure Data Factory (ADF), Logic Apps, and CI/CD practices for smooth data and process integration Python and PySpark: Proficient in Python and PySpark for complex data processing and analysis Main Technologies: Azure Databricks and Spark Ecosystem: Strong knowledge of Azure Databricks, Spark, Spark SQL, PySpark, Delta Tables, and Lakehouse architecture Data Integration with Azure Data Factory: Familiarity with data orchestration and transformation pipelines Nice to Have: Data Lake and Lakehouse Architecture: Experience with Azure Data Lake and a foundational understanding of the Lakehouse paradigm is a plus, with an eagerness to learn more DAX and Power BI for Data Visualization: Background in DAX for data modeling and Power BI for enhanced data insights Azure Fabric and ADF Dataflows: Experience with Azure Fabric for integration and ADF dataflows to enable streamlined data transformations Technical Environment:  Main Technologies or languages: SSIS, DAX, Azure SQL, T-SQL Side Technologies: Azure Logic Apps, Azure Power Automate Main skills: Data Modeling, ETL best practices (incl. incremental patterns, optimizing for huge data loads, coding patterns, DevOps), DW Data modeling, comfortable with connecting to multiple data sources (Oracle, SAP, REST API, Data Lake) Experience : +5 years  Education Level Required : - Bachelor’s Degree  Languages required : English – Advanced level",https://www.linkedin.com/jobs/view/4251791633/?eBP=BUDGET_EXHAUSTED_JOB&refId=Bg0ErupCq9g0G1B89Dz6rA%3D%3D&trackingId=d65CxCzx3rJLXfXAloF1Ow%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:18:41,Data Engineer,0,4252677132,Senior Data Engineer - Apache Flink,Entasis Partners,"Inglaterra, Reino Unido",false,hace 10 horas,38,,,,,,"Acerca del empleo Senior Data Engineer - Apache Flink  *Unfortunately, we are unable to offer visa sponsorship for this role*  Are you a Senior Data Engineer who has hands-on experience working with real-time data processing using Apache Flink? Are you passionate about data engineering, sustainability, and high-impact problem-solving?  Our partner, an innovator in telecommunications, is seeking a Senior Data Engineer with Apache Flink expertise to play a critical role in shaping their data infrastructure as they scale. The Senior Data Engineer will work with innovative technologies such as Apache Flink and be an integral part of their software and data team, owning the architecture, design, and build of their data platform.  Why This Role?  Work in a fast-moving scale-up firm tackling real-world sustainability challenges within the telecommunications industry. Own the architecture, design and build of an innovative data-driven machine learning platform Join a fully remote team (with meetups in London) that values flexibility, family-first culture, and professional growth.  What You'll Be Doing:  Engineering scalable data pipelines to support ML-driven insights from IoT time-series data. Collaborating with data scientists to productionise complex algorithms for robust, high-performance models. Researching and implementing new data technologies to extract deeper insights. Optimising data architecture and access patterns to support the firm's growth plans. Advocating for best practices, continuous learning, and the adoption of new technologies.  What You Bring:  A minimum of 5 years of experience as a Data Engineer. You must have at least 1 year of experience working with Apache Flink. Strong expertise in Python & SQL with a deep understanding of data storage solutions (Postgres, Timescale, DBT). Experience with batch & stream data processing (Kinesis, Spark). Familiarity with DevOps practices (CI/CD, Terraform, GitHub Actions). AWS ecosystem knowledge & UNIX scripting skills. A passion for time-series data, ML-driven solutions, and scaling sustainable technology.  What’s in It for You?  Join a market-defining scale-up in a pivotal growth stage. Work on mission-driven projects that reduce environmental impact. Enjoy remote-first flexibility while staying connected with a collaborative, supportive team. Professional development opportunities, training, and mentorship.  Location: Remote (with occasional meetups in London) Salary: Up to £85,000 + Bonus (up to 10%) Benefits: 25 days holiday | Pension (company: 4%, individual: 5%) | Budget for home office & personal development  Sound good? Apply now!",https://www.linkedin.com/jobs/view/4252677132/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Bg0ErupCq9g0G1B89Dz6rA%3D%3D&trackingId=i4zDrxYHTjJjkMi2MSHp1A%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:18:59,Data Engineer,0,4252672299,AWS Data Engineer,InterEx Group,Suecia,false,hace 11 horas,77,,,,,,"Acerca del empleo This client, we are growing our Data & Analytics footprint. As our new Senior Data Engineer, you will collaborate with product owners, architects and other key profiles across the global organisation helping business verticals and clusters maximise the value of our data and drive business change as an impact of this.  You will be responsible for delivering value-add products, ensuring high success rates as well as be an ambassador that data quality is always a priority. Your primary focus will be designing, building and implementing scalable data products on our global data platform and to secure ownership and stability of the products.  Qualifications and Skills:  The tech stack needed are: Data Engineering Experience using an IDE for ETL Python SQL Cloud (AWS) Big Data Tech Data Modelling & Data Warehouse experience Experience working within scrum and in an agile team  Preferably you would be someone with strong experience in ETL processes and data pipelines.",https://www.linkedin.com/jobs/view/4252672299/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Bg0ErupCq9g0G1B89Dz6rA%3D%3D&trackingId=BziBHWZ3eBoYmT5qp%2FxRrg%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:19:18,Data Engineer,0,4255124502,Big Data Engineer,Ascendion,Polonia,false,hace 13 horas,30,,,,,,"Acerca del empleo Role  Required qualifications  At least 4+years of experience working with Spark with Scala, software design patterns, and TDD. Experience working with big data – Spark, Hadoop, Hive is a must – Azure Databricks be a plus. Agile approach for software development Experience and expertise across data integration and data management with high data volumes. Experiencie working in agile continuous integration/DevOps paradigm and tool set (Git, GitHub, Jenkins, Sonar, Nexus, Jira) Experience with different database structures, including (Postgres, SQL, Hive)  Preferred qualifications Jenkins orchestration Bash script Control-M Software development life cycle (HP ALM,..) Basics of cybersecurity & Quality (Sonar, Fortify…) Basics of Cloud computing (Docker, Kubernetes, OS3, Azure, AWS) SOA Architecture  About Us: Ascendion is a global, leading provider of AI-first software engineering services, delivering transformative solutions across North America, APAC, and Europe. We are headquartered in New Jersey. We combine technology and talent to deliver tech debt relief, improve engineering productivity solutions, and accelerate time to value, driving our clients’ digital journeys with efficiency and velocity. Guided by our “Engineering to the power of AI” [EngineeringAI] methodology, we integrate AI into software engineering, enterprise operations, and talent orchestration, to address critical challenges of trust, speed, and capital. For more information, please go to www.ascendion.com  With Ascendion (www.ascendion.com), you: Will get to work on numerous challenging and exciting projects on our various offerings including Salesforce, AI/Data Science, Generative AI/ML, Automation, Cloud Enterprise and Product/Platform Engineering. At Ascendion you have high chances of project extension or redeployment to other clients. Additionally, you can also share CV of anyone you know. We have a referral policy in place.",https://www.linkedin.com/jobs/view/4255124502/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Bg0ErupCq9g0G1B89Dz6rA%3D%3D&trackingId=Y%2FO3NyCbh3rZ%2BGozcrKNIA%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:19:36,Data Engineer,0,4255193604,AWS Data Engineer,Us3 Consulting,Polonia,false,hace 6 horas,31,,,,,,"Acerca del empleo AWS Data Engineer:  For one of our globally leading tech clients we are urgently looking for an experienced AWS Data Engineer for a long contract based in Poland.   Its paying well and my client can interview asap!   Skills: AWS Data Engineer  Python  SQL  Amazon Web Service(AWS)  Databricks PySpark    If this is you, then please do not delay in applying!",https://www.linkedin.com/jobs/view/4255193604/?eBP=BUDGET_EXHAUSTED_JOB&refId=Bg0ErupCq9g0G1B89Dz6rA%3D%3D&trackingId=QH2Zs5b7HzbdeciPgPcbZA%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:19:51,Data Engineer,0,4255175697,Cloud Data Engineer AWS,Cliente Anônimo®,España,false,hace 8 horas,241,,,,,,"Acerca del empleo Cloud Data Engineer  📍 Ubicación: 100% Remoto 🕓 Tipo de Contrato: Indefinido 💼 Jornada: Completa (salario negociable con base a expertis)  🧩 Descripción del Puesto Cloud Data Engineer con sólida experiencia en soluciones basadas en AWS y un enfoque en el diseño e implementación de arquitecturas de datos modernas. Esta posición está orientada a un perfil técnico con conocimientos avanzados en automatización, visualización de datos y aplicaciones de inteligencia artificial en la nube.  🔧 Responsabilidades:  Implementación de soluciones en la pila de canalización de datos mediante APIs en AWS, incluyendo API Gateway. Desarrollo y despliegue de funciones Lambda en Python para procesamiento de datos. Análisis y documentación de modelos de datos relacionales para generar dashboards y datasets en Amazon Quicksight. Integración de servicios de AWS mediante GitHub Actions. Documentación detallada de arquitecturas de datos en AWS. Desarrollo de soluciones tecnológicas aplicadas a casos de uso de IA y machine learning.  🛠️ Tecnologías y Requisitos (Nivel requerido)  Tecnología-Nivel Requerido  API Management / OpenAPI-Experto Amazon Aurora-Avanzado Amazon CloudWatch-Avanzado Amazon S3- Experto AWS Lambda - Experto Amazon Quicksight-Avanzado Amazon EMR-Avanzado AWS Glue-Avanzado Python - Avanzado SQL (Structured Query Language)- Experto Shell Scripting - Avanzado GitHub (Actions) - Avanzado AI / Machine Learning- Intermedio  ✅ valoramos Experiencia previa en proyectos de datos complejos en entorno AWS. Capacidad para documentar y proponer soluciones arquitectónicas de alto nivel. Iniciativa para aplicar IA en procesos de negocio. Trabajo en entornos DevOps, automatización y CI/CD con GitHub Actions. Excelentes habilidades de comunicación y colaboración en equipos multidisciplinares.  ¿Te interesa el reto? Postúlate ahora o si encuentras cerrada la oferta envía tu CV actualizado a karen.gomez@sectorea.com con el asunto ""Cloud Data Engineer""  🎯 ¿Qué ofrecemos?  ✔Contrato Indefinido en una compañía 100% digital tecnológica y del IBEX 35.   ✔Teletrabajo 100%, o si lo prefieres, modalidad híbrida en tu provincia.   ✔Horario flexible e intensivo (todos los viernes y los meses de verano, Julio y Agosto).   ✔ Salario abierto a escuchar motivación al cambio del candidato, en función expertise.   ✔ Posibilidad de trabajar con los expertos referentes en proyectos tecnológicamente punteros que generan un impacto real en la sociedad.   ✔Beneficios adicionales al salario: seguro de vida, servicios de telemedicina y asistencia legal.   ✔Podrás disfrutar de 32 días de vacaciones y laborables de descanso al año: 22 normal + 10 de libre disposición.   ✔ Formación continuada y personalizada, miles de cursos especializados en IT en Udemy for Business. Además si quieres certificarte te vamos a apoyar.   ✔Acceso a plataforma MyClub donde te podrás beneficiar de numerosos descuentos y promociones exclusivos para nuestros profesionales.   ✔Favorecemos que puedas asistir a eventos tanto nacionales como internacionales y organizaremos eventos open Company, así como participar en las iniciativas y actividades de la compañía.   ✔Acceso a plataforma Privilege Store donde te podrás beneficiar de numerosos descuentos y promociones exclusivos para nuestros profesionales.   ✔Y también, puedes acceder a un programa de retribución flexible que te hará la vida más cómoda: Tarjeta de transporte público, tarjeta restaurante, Cheques guardería, seguro médico privado y plan de previsión social.   ✔Por otro lado, desarrollan modelos de intraemprendimiento, siendo vehículo de inversión de startups, spinoffs e iniciativas innovadoras de base tecnológica. Si tienes una idea de negocio, la podrás desarrollar con nosotros.   ✔Trabajarás con los referentes tecnológicos de una multinacional española líder en transformación digital.   ✔Participaras en proyectos retadores que generan un impacto positivo en la sociedad, creando soluciones que integran negocio y tecnología.",https://www.linkedin.com/jobs/view/4255175697/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Bg0ErupCq9g0G1B89Dz6rA%3D%3D&trackingId=23cWkv1a2ZCDBYzJkgA0GA%3D%3D&trk=flagship3_search_srp_jobs
23Jun2025-17:20:08,Data Engineer,0,4253307318,Graph Database Engineer,THOUGHT BYTE,Estados Unidos,false,hace 5 horas,42,,,,,,"Acerca del empleo Position: Graph DB Engineer Location: 100% Remote Job Type: FTE (Preferred) Or FTC- W2  Who are we looking for? We are seeking an experienced and passionate Graph Database Engineer with 6+ years of expertise in graph databases . As a key member of our team, you will contribute to cutting-edge projects, leveraging your proven experience in graph database design, development, and optimization. Your expertise will drive innovation and efficiency, ensuring seamless data management and retrieval. Collaborating closely with cross-functional teams, you’ll contribute to the success of our projects and elevate technical capabilities.  Responsibilities:  Skills – Must have: · Translate business requirements into technical solutions using graph databases. · Design and develop Graph database solutions on business requirements. · Create and optimize graph database models/schemas for querying and traversal. · Experience on RDF based graph databases. · Implement indexing and caching strategies for data retrieval. · Extensive work experience on knowledge graph implementation. · Ensure scalability, reliability, and performance of graph databases. · Fine-tune graph queries for optimal performance. · Monitor and troubleshoot graph database performance metrics. · Handle schema migrations and version upgrades. · Manage database security, access controls, and backups. · Collaborate with cross-functional teams to integrate graph databases into application ecosystem. · Work closely with data scientists, software developers, and product managers to align graph database solutions with overall project goals. · Guide junior developers on technical implementation. · Strong understanding of graph theory, traversal algorithms, and data structures. · Experience with data modeling, development and analytics implementation. · Familiarity with industry standards and best practices on Graph implementation · Proficiency in graph database technologies (e.g., rdf4j , OrientDB, Neo4j , Nebula, ArangoDB, GraphBase, Neptune, TigerGraph, etc.)  Skills – Good to have: · AI/ML exposure. · Python programming · Analytics exposure  EDUCATION QUALIFICATION · Graduate in Engineering OR Masters in Computer Applications.  Process Skills: · General SDLC processes · Understanding of Agile and Scrum software development methodologies · Skill in gathering and documenting user requirements and writing technical specifications.  Behavioral Skills : · Good Attitude and Quick learner. · Well-developed design, analytical & problem-solving skills · Strong oral and written communication skills · Excellent team player, able to work with virtual teams. · Self-motivated and capable of working independently with minimal management supervision.  Certification (Good to have): · Any Graph Certification",https://www.linkedin.com/jobs/view/4253307318/?eBP=BUDGET_EXHAUSTED_JOB&refId=Bg0ErupCq9g0G1B89Dz6rA%3D%3D&trackingId=5MGzLDZwZyO2vveE13SWbQ%3D%3D&trk=flagship3_search_srp_jobs
